"""
Auditor Module - Adversarial Debate System
ë‹¤ê°ë„ ë²•ë¥  ë¶„ì„ ë° íŒê²° ëª¨ë“ˆ (Prosecutor vs Defense vs Judge)
"""
import asyncio
import json_repair
from typing import Tuple

from models import Scenario, LegalEvidence, RiskReport
from llm_client import llm_client


class AdversarialDebate:
    """
    Multi-Agent Debate System: Prosecutor vs. Defense -> Judge
    Includes Rebuttal & Reflexion (Self-Correction) phases.
    """

    PROSECUTOR_PROMPT = """
    You are a Legal Risk Assessment Specialist.
    Based on the scenario and evidence, identify ALL potential legal risks, liabilities, and violations.
    
    [Hierarchy of Evidence]
    1. **Primary Authority**: Statutes (Acts, Decrees) and Administrative Rules.
    2. **Secondary Reference**: Precedents (Case Law). Use these only to support statutory interpretation.
    
    [Scenario]
    {scenario}

    [Evidence]
    {evidence}
    
    Focus on:
    - Specific violations of Statutes/Rules
    - Civil/Criminal liabilities based on these laws
    - Administrative sanctions (fines, license revocation)
    - If precedents are scarce, rely on the **text of the law** and **legal principles**.
    - DO NOT overemphasize the lack of precedents as a risk itself.
    """

    DEFENSE_PROMPT = """
    You are a Legal Rights Advocate.
    Based on the scenario and evidence, identify legal protections and rights.

    [Hierarchy of Evidence]
    1. **Primary Authority**: Statutes (Acts, Decrees) and Administrative Rules.
    2. **Secondary Reference**: Precedents (Case Law).
    
    [Scenario]
    {scenario}
    
    [Evidence]
    {evidence}
    
    Focus on:
    - Exceptions or protections defined in Statutes/Rules
    - Rights guaranteed by law
    - Interpretation of legal text in favor of the client
    - If precedents are scarce, argue based on **statutory intent** and **fairness**.
    """

    REBUTTAL_PROMPT = """
    You are {role}.
    Critique the opponent's argument logically.
    Opponent Argument:
    {opponent_argument}
    
    Language: English.
    """

    REFLEXION_PROMPT = """
    You are {role}.
    Refine your final argument.
    My Original Argument: {my_argument}
    Opponent's Rebuttal: {rebuttal}
    
    Language: English.
    """

    JUDGE_PROMPT = """
    You are a 'Friendly and Professional Legal Counsel' (ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ë³€í˜¸ì‚¬).
    Your goal is to explain complex legal issues to a non-expert client in an easy-to-understand and warm manner.
    
    [Input Data]
    1. Legal Situation: {scenario}
    2. Collected Legal Evidence: {evidence}
    3. Risk Analysis (Legal Concerns): {prosecutor_final}
    4. Rights Analysis (Legal Protections): {defense_final}

    [Tone & Style Guidelines]
    1. **Persona**: A trustworthy, warm, and highly competent lawyer.
    2. **Language**: Polite Korean (Honorifics, e.g., "ê²€í† í•´ ë³´ì•˜ìŠµë‹ˆë‹¤", "íŒë‹¨ë©ë‹ˆë‹¤", "ì¶”ì²œë“œë¦½ë‹ˆë‹¤").
    3. **Clarity**: Avoid overly difficult legal jargon where possible, or explain it simply.
    4. **Empathy**: Acknowledge the user's situation before delivering the legal verdict.

    [Judgment Guidelines]
    1. **Statute-Centric**: Base your advice primarily on Acts, Decrees, and Rules.
    2. **Precedents**: Use precedents as supporting examples to explain *how* the law is applied.
    3. **Missing Evidence**: If no specific law is found, explain general legal principles instead of saying "I don't know."

    [Task]
    Provide a comprehensive legal advisory opinion.
    
    [Output JSON (Korean)]
    {{
        "ìœ„í—˜ë„": "ì•ˆì „ | ì£¼ì˜ | ìœ„í—˜",
        "ì •í™•ë„": 0 ~ 100,
        "í‰ê°€ë‚´ìš©": "ì¹œì ˆí•œ ë³€í˜¸ì‚¬ ë§íˆ¬ë¡œ ì‘ì„±ëœ ì¢…í•© ìë¬¸ ì˜ê²¬.\\n\\nì•ˆë…•í•˜ì„¸ìš”, MIRI ë²•ë¥  ìë¬¸ì…ë‹ˆë‹¤. ì˜ë¢°í•˜ì‹  ë‚´ìš©ì„ ê¼¼ê¼¼íˆ ê²€í† í•´ ë³´ì•˜ìŠµë‹ˆë‹¤.\\n\\n1. **ìƒí™© ë¶„ì„**: (ì˜ë¢°ì¸ì˜ ìƒí™©ì„ ê³µê°í•˜ë©° ìš”ì•½)\\n2. **ë²•ì  ê²€í† **: (ê´€ë ¨ ë²•ë ¹ê³¼ í–‰ì •ê·œì¹™ì„ ê·¼ê±°ë¡œ ìœ„ë²•/ì ë²• ì—¬ë¶€ë¥¼ ì‰½ê²Œ ì„¤ëª…)\\n3. **íŒë¡€ ê²½í–¥**: (ê´€ë ¨ íŒë¡€ê°€ ìˆë‹¤ë©´ 'ì´ëŸ° ê²½ìš°ì—ëŠ” ë²•ì›ì´ ì´ë ‡ê²Œ íŒë‹¨í•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤'ë¼ê³  ì†Œê°œ)\\n4. **ëŒ€ì‘ ë°©ì•ˆ**: (ì˜ë¢°ì¸ì´ ì·¨í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ í–‰ë™ì´ë‚˜ ê¶Œë¦¬ êµ¬ì œ ë°©ì•ˆ ì œì•ˆ)\\n5. **ì¢…í•© ê²°ë¡ **: (ìµœì¢…ì ì¸ ì¡°ì–¸ê³¼ í•¨ê²Œ ë§ˆë¬´ë¦¬ ì¸ì‚¬)",
        "ì¸ìš©ê·¼ê±°": ["ê·¼ë¡œê¸°ì¤€ë²• ì œ23ì¡°", "ëŒ€ë²•ì› 20xxë‹¤xxxxx (ì°¸ê³ )", ...],
        "í‰ê°€ê²°ê³¼": "ë¶€ë‹¹í•´ê³  êµ¬ì œ ì‹ ì²­ ê°€ëŠ¥ | ê³„ì•½ì„œ ìˆ˜ì • ê¶Œê³  | ë²•ì  ë¦¬ìŠ¤í¬ ë‚®ìŒ ë“± (ì§§ì€ ìš”ì•½)",
        "ì£¼ìš”ìŸì ": ["í•´ê³  ì˜ˆê³  ì˜ë¬´ ìœ„ë°˜ ì—¬ë¶€", "ì •ë‹¹í•œ í•´ê³  ì‚¬ìœ  ì¡´ì¬ ì—¬ë¶€", ...]
    }}
    """

    async def _opening_statements(self, context: dict) -> Tuple[str, str]:
        print("    âš”ï¸ [Round 1] Opening Statements...")
        pros_task = llm_client.generate("", self.PROSECUTOR_PROMPT.format(**context), model="gpt-4o-mini")
        def_task = llm_client.generate("", self.DEFENSE_PROMPT.format(**context), model="gpt-4o-mini")
        
        pros_arg, def_arg = await asyncio.gather(pros_task, def_task)
        return pros_arg.strip(), def_arg.strip()

    async def _rebuttal_round(self, pros_arg: str, def_arg: str) -> Tuple[str, str]:
        print("    âš”ï¸ [Round 2] Rebuttal (Cross-Examination)...")
        p_rebut_task = llm_client.generate(self.REBUTTAL_PROMPT.format(role="Prosecutor", opponent_argument=def_arg), "", model="gpt-4o-mini")
        d_rebut_task = llm_client.generate(self.REBUTTAL_PROMPT.format(role="Defense Lawyer", opponent_argument=pros_arg), "", model="gpt-4o-mini")

        p_rebut, d_rebut = await asyncio.gather(p_rebut_task, d_rebut_task)
        return p_rebut.strip(), d_rebut.strip()

    async def _reflexion_round(self, pros_arg: str, def_arg: str, p_rebut: str, d_rebut: str) -> Tuple[str, str]:
        print("    ğŸ§  [Round 3] Reflexion (Self-Correction)...")
        p_final_task = llm_client.generate(self.REFLEXION_PROMPT.format(role="Prosecutor", my_argument=pros_arg, rebuttal=d_rebut), "", model="gpt-4o-mini")
        d_final_task = llm_client.generate(self.REFLEXION_PROMPT.format(role="Defense Lawyer", my_argument=def_arg, rebuttal=p_rebut), "", model="gpt-4o-mini")

        p_final, d_final = await asyncio.gather(p_final_task, d_final_task)
        return p_final.strip(), d_final.strip()

    async def _render_verdict(self, scenario_text: str, p_final: str, d_final: str, evidence_text: str) -> RiskReport:
        print("    âš–ï¸ [Judge] Rendering Final Verdict...")
        prompt = self.JUDGE_PROMPT.format(
            scenario=scenario_text,
            prosecutor_final=p_final,
            defense_final=d_final,
            evidence=evidence_text
        )

        response = await llm_client.generate("", prompt, model="gpt-4o-mini", max_tokens=2048)

        try:
            data = json_repair.loads(response)

            # í•œêµ­ì–´ í•„ë“œëª… ë§¤í•‘ (LLMì´ í•œêµ­ì–´ë¡œ ì‘ë‹µ)
            risk_level = data.get('ìœ„í—˜ë„', data.get('risk_level', 'Caution'))
            confidence = data.get('ì •í™•ë„', data.get('confidence_score', 0))
            verdict_text = data.get('í‰ê°€ë‚´ìš©', data.get('verdict', 'í‰ê°€ ë‚´ìš© ì—†ìŒ'))
            cited = data.get('ì¸ìš©ê·¼ê±°', data.get('cited_evidence', []))
            winning = data.get('í‰ê°€ê²°ê³¼', data.get('winning_side', 'í‰ê°€ ì¤‘'))
            issues = data.get('ì£¼ìš”ìŸì ', data.get('key_issues', []))

            # ì¸ìš© ê·¼ê±° í¬ë§·íŒ…
            if isinstance(cited, list):
                citation_text = "\n".join(cited)
            else:
                citation_text = str(cited)

            # ìœ„í—˜ë„ ì˜ë¬¸ ë§¤í•‘ (í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜)
            risk_map = {'ì•ˆì „': 'Safe', 'ì£¼ì˜': 'Caution', 'ìœ„í—˜': 'Danger'}
            risk_level_en = risk_map.get(risk_level, risk_level)

            return RiskReport(
                verdict=risk_level_en,
                summary=f"[{winning}] {verdict_text} (ì •í™•ë„: {confidence}%)",
                citation=citation_text,
                key_issues=issues
            )
        except Exception as e:
            print(f"Judge Error: {e}")
            return RiskReport(
                verdict="Caution", 
                summary=f"í‰ê°€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}", 
                citation="", 
                key_issues=["ì‹œìŠ¤í…œ ì˜¤ë¥˜"]
            )

    async def execute(self, scenario: Scenario, evidence: LegalEvidence) -> RiskReport:
        evidence_text = "\n".join(evidence.relevant_laws)
        if not evidence_text: evidence_text = "No specific laws found."

        context = {
            "scenario": scenario.model_dump_json(),
            "evidence": evidence_text
        }

        # 1. Opening
        p_arg, d_arg = await self._opening_statements(context)
        print(f"      ğŸ—£ï¸ Prosecutor: {p_arg[:100]}...")
        print(f"      ğŸ›¡ï¸ Defense: {d_arg[:100]}...")

        # 2. Rebuttal
        p_rebut, d_rebut = await self._rebuttal_round(p_arg, d_arg)

        # 3. Reflexion
        p_final, d_final = await self._reflexion_round(p_arg, d_arg, p_rebut, d_rebut)
        print(f"      ğŸ“ Pros Final: {p_final[:100]}...")
        print(f"      ğŸ“ Def Final: {d_final[:100]}...")

        # 4. Verdict
        return await self._render_verdict(scenario.model_dump_json(), p_final, d_final, evidence_text)
