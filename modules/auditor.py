"""
Auditor Module - Adversarial Debate System
ë‹¤ê°ë„ ë²•ë¥  ë¶„ì„ ë° íŒê²° ëª¨ë“ˆ (Prosecutor vs Defense vs Judge)
"""
import asyncio
import json_repair
from typing import Tuple

from models import Scenario, LegalEvidence, RiskReport
from llm_client import llm_client


class AdversarialDebate:
    """
    Multi-Agent Debate System: Prosecutor vs. Defense -> Judge
    Includes Rebuttal & Reflexion (Self-Correction) phases.
    """

    PROSECUTOR_PROMPT = """
    You are a Legal Risk Assessment Specialist.
    Based on the scenario and evidence, identify ALL potential legal risks, liabilities, and violations.
    
    [Hierarchy of Evidence]
    1. **Primary Authority**: Statutes (Acts, Decrees) and Administrative Rules.
    2. **Secondary Reference**: Precedents (Case Law). Use these only to support statutory interpretation.
    
    [Scenario]
    {scenario}

    [Evidence]
    {evidence}
    
    Focus on:
    - Specific violations of Statutes/Rules
    - Civil/Criminal liabilities based on these laws
    - Administrative sanctions (fines, license revocation)
    - If precedents are scarce, rely on the **text of the law** and **legal principles**.
    - DO NOT overemphasize the lack of precedents as a risk itself.
    """

    DEFENSE_PROMPT = """
    You are a Legal Rights Advocate.
    Based on the scenario and evidence, identify legal protections and rights.

    [Hierarchy of Evidence]
    1. **Primary Authority**: Statutes (Acts, Decrees) and Administrative Rules.
    2. **Secondary Reference**: Precedents (Case Law).
    
    [Scenario]
    {scenario}
    
    [Evidence]
    {evidence}
    
    Focus on:
    - Exceptions or protections defined in Statutes/Rules
    - Rights guaranteed by law
    - Interpretation of legal text in favor of the client
    - If precedents are scarce, argue based on **statutory intent** and **fairness**.
    """

    REBUTTAL_PROMPT = """
    You are {role}.
    Critique the opponent's argument logically.
    Opponent Argument:
    {opponent_argument}
    
    Language: English.
    """

    REFLEXION_PROMPT = """
    You are {role}.
    Refine your final argument.
    My Original Argument: {my_argument}
    Opponent's Rebuttal: {rebuttal}
    
    Language: English.
    """

    JUDGE_PROMPT = """
    You are a 'Senior Legal Counsel' responsible for comprehensive legal advisory.
    
    [Input Data]
    1. Legal Situation: {scenario}
    2. Collected Legal Evidence: {evidence}
    3. Risk Analysis (Legal Concerns): {prosecutor_final}
    4. Rights Analysis (Legal Protections): {defense_final}

    [Judgment Guidelines]
    1. **Statute-Centric Approach**: Base your verdict PRIMARILY on Acts, Decrees, and Administrative Rules.
    2. **Precedents as Reference**: Use precedents ONLY to illustrate past applications. Do NOT treat them as absolute rules if the statute is clear.
    3. **Handling Missing Precedents**: If no specific precedent exists, DO NOT warn about "uncertainty" or "risk of bias". Instead, interpret the **Statutory Text** directly.
    4. **Specific Citation**: Always cite the specific Article/Clause of the Law (e.g., "Road Traffic Act Art. 54").

    [Task]
    Provide a comprehensive legal advisory opinion.
    - Adapt your analysis to the consultation type (Business, Contract, Dispute, Daily Life).
    - If the law is clear, give a definitive answer.

    [Output JSON (Korean)]
    {{
        "ìœ„í—˜ë„": "ì•ˆì „ | ì£¼ì˜ | ìœ„í—˜",
        "ì •í™•ë„": 0 ~ 100,
        "í‰ê°€ë‚´ìš©": "ì¢…í•© ë²•ë¥  ìë¬¸ ì˜ê²¬.\\n\\n1. ìƒí™© ë¶„ì„: í•µì‹¬ ìŸì  ìš”ì•½.\\n2. ë²•ë ¹ ê²€í†  (í•µì‹¬): ê´€ë ¨ ë²•ë ¹ ë° í–‰ì •ê·œì¹™ì— ê·¼ê±°í•œ ìœ„ë²•ì„±/ì ë²•ì„± íŒë‹¨. (ê°€ì¥ ì¤‘ìš”)\\n3. íŒë¡€ ê²½í–¥ (ì°¸ê³ ): ê´€ë ¨ íŒë¡€ê°€ ìˆë‹¤ë©´ 'ì°¸ê³ ì ìœ¼ë¡œ' ì–¸ê¸‰. ì—†ìœ¼ë©´ ìƒëµí•˜ê±°ë‚˜ ì¼ë°˜ ì›ì¹™ ì„œìˆ .\\n4. ê¶Œë¦¬ ë° êµ¬ì œ: ì˜ë¢°ì¸ì˜ ê¶Œë¦¬ì™€ ëŒ€ì‘ ë°©ì•ˆ.\\n5. ê²°ë¡ : ìµœì¢… ì˜ê²¬ ë° í–‰ë™ ì§€ì¹¨.",
        "ì¸ìš©ê·¼ê±°": ["ë²•ë ¹ëª… ì œOì¡°", "íŒë¡€: 20xxë‹¤xxxxx (ì°¸ê³ )", ...],
        "í‰ê°€ê²°ê³¼": "ê·œì œ ìƒŒë“œë°•ìŠ¤ ì‹ ì²­ ê¶Œì¥ | ë²•ì  ëŒ€ì‘ í•„ìš” | ê³„ì•½ í•´ì œ ê°€ëŠ¥ | ì†í•´ë°°ìƒì²­êµ¬ ê²€í†  ë“±",
        "ì£¼ìš”ìŸì ": ["ìŸì 1: [í–‰ìœ„] -> [ë²•ë ¹] ìœ„ë°˜ ì—¬ë¶€", ...]
    }}
    """

    async def _opening_statements(self, context: dict) -> Tuple[str, str]:
        print("    âš”ï¸ [Round 1] Opening Statements...")
        pros_task = llm_client.generate("", self.PROSECUTOR_PROMPT.format(**context), model="gpt-4o-mini")
        def_task = llm_client.generate("", self.DEFENSE_PROMPT.format(**context), model="gpt-4o-mini")
        
        pros_arg, def_arg = await asyncio.gather(pros_task, def_task)
        return pros_arg.strip(), def_arg.strip()

    async def _rebuttal_round(self, pros_arg: str, def_arg: str) -> Tuple[str, str]:
        print("    âš”ï¸ [Round 2] Rebuttal (Cross-Examination)...")
        p_rebut_task = llm_client.generate(self.REBUTTAL_PROMPT.format(role="Prosecutor", opponent_argument=def_arg), "", model="gpt-4o-mini")
        d_rebut_task = llm_client.generate(self.REBUTTAL_PROMPT.format(role="Defense Lawyer", opponent_argument=pros_arg), "", model="gpt-4o-mini")

        p_rebut, d_rebut = await asyncio.gather(p_rebut_task, d_rebut_task)
        return p_rebut.strip(), d_rebut.strip()

    async def _reflexion_round(self, pros_arg: str, def_arg: str, p_rebut: str, d_rebut: str) -> Tuple[str, str]:
        print("    ğŸ§  [Round 3] Reflexion (Self-Correction)...")
        p_final_task = llm_client.generate(self.REFLEXION_PROMPT.format(role="Prosecutor", my_argument=pros_arg, rebuttal=d_rebut), "", model="gpt-4o-mini")
        d_final_task = llm_client.generate(self.REFLEXION_PROMPT.format(role="Defense Lawyer", my_argument=def_arg, rebuttal=p_rebut), "", model="gpt-4o-mini")

        p_final, d_final = await asyncio.gather(p_final_task, d_final_task)
        return p_final.strip(), d_final.strip()

    async def _render_verdict(self, scenario_text: str, p_final: str, d_final: str, evidence_text: str) -> RiskReport:
        print("    âš–ï¸ [Judge] Rendering Final Verdict...")
        prompt = self.JUDGE_PROMPT.format(
            scenario=scenario_text,
            prosecutor_final=p_final,
            defense_final=d_final,
            evidence=evidence_text
        )

        response = await llm_client.generate("", prompt, model="gpt-4o-mini", max_tokens=2048)

        try:
            data = json_repair.loads(response)

            # í•œêµ­ì–´ í•„ë“œëª… ë§¤í•‘ (LLMì´ í•œêµ­ì–´ë¡œ ì‘ë‹µ)
            risk_level = data.get('ìœ„í—˜ë„', data.get('risk_level', 'Caution'))
            confidence = data.get('ì •í™•ë„', data.get('confidence_score', 0))
            verdict_text = data.get('í‰ê°€ë‚´ìš©', data.get('verdict', 'í‰ê°€ ë‚´ìš© ì—†ìŒ'))
            cited = data.get('ì¸ìš©ê·¼ê±°', data.get('cited_evidence', []))
            winning = data.get('í‰ê°€ê²°ê³¼', data.get('winning_side', 'í‰ê°€ ì¤‘'))
            issues = data.get('ì£¼ìš”ìŸì ', data.get('key_issues', []))

            # ì¸ìš© ê·¼ê±° í¬ë§·íŒ…
            if isinstance(cited, list):
                citation_text = "\n".join(cited)
            else:
                citation_text = str(cited)

            # ìœ„í—˜ë„ ì˜ë¬¸ ë§¤í•‘ (í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜)
            risk_map = {'ì•ˆì „': 'Safe', 'ì£¼ì˜': 'Caution', 'ìœ„í—˜': 'Danger'}
            risk_level_en = risk_map.get(risk_level, risk_level)

            return RiskReport(
                verdict=risk_level_en,
                summary=f"[{winning}] {verdict_text} (ì •í™•ë„: {confidence}%)",
                citation=citation_text,
                key_issues=issues
            )
        except Exception as e:
            print(f"Judge Error: {e}")
            return RiskReport(
                verdict="Caution", 
                summary=f"í‰ê°€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}", 
                citation="", 
                key_issues=["ì‹œìŠ¤í…œ ì˜¤ë¥˜"]
            )

    async def execute(self, scenario: Scenario, evidence: LegalEvidence) -> RiskReport:
        evidence_text = "\n".join(evidence.relevant_laws)
        if not evidence_text: evidence_text = "No specific laws found."

        context = {
            "scenario": scenario.model_dump_json(),
            "evidence": evidence_text
        }

        # 1. Opening
        p_arg, d_arg = await self._opening_statements(context)
        print(f"      ğŸ—£ï¸ Prosecutor: {p_arg[:100]}...")
        print(f"      ğŸ›¡ï¸ Defense: {d_arg[:100]}...")

        # 2. Rebuttal
        p_rebut, d_rebut = await self._rebuttal_round(p_arg, d_arg)

        # 3. Reflexion
        p_final, d_final = await self._reflexion_round(p_arg, d_arg, p_rebut, d_rebut)
        print(f"      ğŸ“ Pros Final: {p_final[:100]}...")
        print(f"      ğŸ“ Def Final: {d_final[:100]}...")

        # 4. Verdict
        return await self._render_verdict(scenario.model_dump_json(), p_final, d_final, evidence_text)
