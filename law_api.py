"""
National Law API Client for MIRI Legal Advisory System
í•œêµ­ ë²•ì œì²˜ API í´ë¼ì´ì–¸íŠ¸
"""
import re
import asyncio
import aiohttp
import xmltodict
from urllib.parse import urlencode, urlparse, parse_qs, urlunparse
from typing import Any, List, Dict, Tuple
from config import MAX_SEARCH_RESULTS_PER_SOURCE


class NationalLawAPI:
    def __init__(self, api_id="jaeyeongm34", base_url="http://www.law.go.kr"):
        self.base_url = base_url
        self.api_id = api_id
        self.is_mock = not bool(self.api_id)
        self._cache = {}
        self.semaphore = asyncio.Semaphore(5)  # ë™ì‹œ ìš”ì²­ ì œí•œ

    def _force_list(self, data: Any) -> List[Any]:
        if not data: return []
        if isinstance(data, list): return data
        return [data]

    async def _fetch(self, url: str) -> Dict[str, Any]:
        if url in self._cache:
            return self._cache[url]
        
        async with self.semaphore:
            async with aiohttp.ClientSession() as session:
                try:
                    async with session.get(url) as response:
                        if response.status != 200:
                            print(f"    âŒ API Error: Status {response.status} for {url}")
                            return {}
                        content = await response.text()
                        parsed = xmltodict.parse(content)
                        self._cache[url] = parsed
                        return parsed
                except Exception as e:
                    print(f"    âŒ Fetch Exception: {e}")
                    return {}

    async def search_list(self, target: str, query: str, **kwargs) -> List[Dict[str, Any]]:
        if self.is_mock: return []

        endpoint = "lawSearch.do"
        params = {
            "OC": self.api_id,
            "target": target,
            "type": "XML",
            "query": query,
            "display": kwargs.get('display', MAX_SEARCH_RESULTS_PER_SOURCE),
            "nw": 3  # ê¸°ë³¸ê°’: í˜„í–‰ ë²•ë ¹ë§Œ ê²€ìƒ‰
        }
        params.update(kwargs)  # JO ë“± ì¶”ê°€ íŒŒë¼ë¯¸í„° ë³‘í•©

        # ë””ë²„ê¹…: ì‹¤ì œ ìš”ì²­ URL íŒŒë¼ë¯¸í„° ì¶œë ¥
        if target == 'prec':
            jo_param = kwargs.get('JO', '')
            print(f"      ğŸ“¡ [API ìš”ì²­] {target.upper()} ê²€ìƒ‰ | Query='{query}' | JO='{jo_param}'")
        else:
            print(f"      ğŸ“¡ [API ìš”ì²­] {target.upper()} ê²€ìƒ‰ | Query='{query}'")

        query_string = urlencode(params, doseq=True)
        url = f"{self.base_url}/DRF/{endpoint}?{query_string}"

        data = await self._fetch(url)

        # eflaw ì§€ì› ì¶”ê°€
        root_map = {'law': 'LawSearch', 'eflaw': 'LawSearch', 'admrul': 'AdmRulSearch', 'prec': 'PrecSearch'}
        root_key = root_map.get(target, 'LawSearch')

        try:
            search_data = data.get(root_key, {})
            item_key = 'law' if target == 'eflaw' else target

            result = search_data.get(item_key, [])
            items = self._force_list(result)
            print(f"        -> âœ… ê²°ê³¼: {len(items)}ê±´ ë°œê²¬")
            return items
        except AttributeError:
            print(f"        -> âš ï¸ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨ ë˜ëŠ” 0ê±´")
            return []

    def _clean_html(self, text: str) -> str:
        """HTML íƒœê·¸ ì œê±° + ë¶ˆí•„ìš”í•œ ë©”íƒ€ë°ì´í„° ì •ë¦¬"""
        if not text: return ""
        
        # 1. HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text).strip()
        
        # 2. ê°œì • ì´ë ¥ ì œê±°
        text = re.sub(r'<(ê°œì •|ì‹ ì„¤|ì „ë¬¸ê°œì •|íƒ€ë²•ê°œì •|ì¼ë¶€ê°œì •|íì§€)\s+[\d.,\s]+>', '', text)
        
        # 3. ì°¸ê³  ì •ë³´ ì œê±°
        text = re.sub(r'\[(ì „ë¬¸ê°œì •|ê°œì •|ì‹ ì„¤|íƒ€ë²•ê°œì •|ì¼ë¶€ê°œì •|íì§€)\s+[\d.,\s]+\]', '', text)
        
        # 4. ì¥/ì ˆ/ê´€ í—¤ë” ì œê±°
        text = re.sub(r'ì œ\d+ì¥\s+[ê°€-í£\s]+', '', text)
        text = re.sub(r'ì œ\d+ì ˆ\s+[ê°€-í£\s]+', '', text)
        
        # 5. ë‹¤ì¤‘ ê³µë°±/ê°œí–‰ ì •ë¦¬
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text

    def _parse_xml_to_text(self, data: Dict[str, Any]) -> str:
        # 1. ë²•ë ¹ (Law)
        if 'ë²•ë ¹' in data:
            root = data['ë²•ë ¹']
            text_parts = []
            
            title = root.get('ê¸°ë³¸ì •ë³´', {}).get('ë²•ë ¹ëª…_í•œê¸€', '')
            text_parts.append(f"== {title} ==\n")

            jo_list = self._force_list(root.get('ì¡°ë¬¸', {}).get('ì¡°ë¬¸ë‹¨ìœ„', []))
            for jo in jo_list:
                jo_content = self._clean_html(jo.get('ì¡°ë¬¸ë‚´ìš©', ''))
                item_text = [jo_content]
                
                hang_list = self._force_list(jo.get('í•­', []))
                for hang in hang_list:
                    h_content = self._clean_html(hang.get('í•­ë‚´ìš©', ''))
                    if h_content:
                        item_text.append(f"  {h_content}")
                    
                    ho_list = self._force_list(hang.get('í˜¸', []))
                    for ho in ho_list:
                        ho_content = self._clean_html(ho.get('í˜¸ë‚´ìš©', ''))
                        if ho_content:
                            item_text.append(f"    {ho_content}")
                
                text_parts.append("\n".join(item_text))
            
            return "\n\n".join(text_parts) if len(text_parts) > 1 else str(data)

        # 2. í–‰ì •ê·œì¹™ (AdmRul)
        if 'í–‰ì •ê·œì¹™' in data:
            root = data['í–‰ì •ê·œì¹™']
            text_parts = []
            
            title = root.get('ê¸°ë³¸ì •ë³´', {}).get('í–‰ì •ê·œì¹™ëª…', '')
            text_parts.append(f"== {title} ==\n")
            
            jo_list = self._force_list(root.get('ì¡°ë¬¸', {}).get('ì¡°ë¬¸ë‹¨ìœ„', []))
            if jo_list:
                for jo in jo_list:
                    jo_content = self._clean_html(jo.get('ì¡°ë¬¸ë‚´ìš©', ''))
                    text_parts.append(jo_content)
                    
                    hang_list = self._force_list(jo.get('í•­', []))
                    for hang in hang_list:
                        h_content = self._clean_html(hang.get('í•­ë‚´ìš©', ''))
                        if h_content: text_parts.append(f"  {h_content}")
            else:
                text_parts.append(self._clean_html(root.get('ë³¸ë¬¸', '')))

            return "\n\n".join(text_parts)

        # 3. íŒë¡€ (Prec)
        if 'íŒë¡€' in data:
            root = data['íŒë¡€']
            issue = self._clean_html(root.get('íŒì‹œì‚¬í•­', ''))
            summary = self._clean_html(root.get('íŒê²°ìš”ì§€', ''))
            content = self._clean_html(root.get('íŒë¡€ë‚´ìš©', ''))
            
            return f"[íŒì‹œì‚¬í•­]\n{issue}\n\n[íŒê²°ìš”ì§€]\n{summary}\n\n[íŒë¡€ë‚´ìš©]\n{content}"

        return str(data)

    def _get_unique_id(self, data: Dict[str, Any]) -> str:
        """XML ë°ì´í„°ì—ì„œ ê³ ìœ  ì‹ë³„ì ì¶”ì¶œ"""
        if 'ë²•ë ¹' in data:
            return str(data['ë²•ë ¹'].get('ê¸°ë³¸ì •ë³´', {}).get('ë²•ë ¹ID', 'Unknown'))
        if 'í–‰ì •ê·œì¹™' in data:
            return str(data['í–‰ì •ê·œì¹™'].get('ê¸°ë³¸ì •ë³´', {}).get('í–‰ì •ê·œì¹™ì¼ë ¨ë²ˆí˜¸', 'Unknown'))
        if 'íŒë¡€' in data:
            return str(data['íŒë¡€'].get('íŒë¡€ì •ë³´ì¼ë ¨ë²ˆí˜¸', 'Unknown'))
        return "Unknown"

    def _parse_law_structure(self, data: Dict[str, Any]) -> List[Dict[str, str]]:
        """XML êµ¬ì¡°ë¥¼ í™œìš©í•´ ë²•ë ¹ì˜ [ì¡°ë¬¸/ë³„í‘œ] ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ"""
        articles = []
        
        # 1. ë²•ë ¹ (Law)
        if 'ë²•ë ¹' in data:
            root = data['ë²•ë ¹']
            
            # (1) ì¡°ë¬¸ íŒŒì‹±
            jo_list = self._force_list(root.get('ì¡°ë¬¸', {}).get('ì¡°ë¬¸ë‹¨ìœ„', []))
            for jo in jo_list:
                jo_type = jo.get('ì¡°ë¬¸ì—¬ë¶€', '')
                if jo_type == 'ì „ë¬¸':
                    continue
                
                jo_text = self._clean_html(jo.get('ì¡°ë¬¸ë‚´ìš©', ''))
                
                if not jo_text or len(jo_text) < 5:
                    continue
                
                match = re.match(r'(ì œ\d+ì¡°ì˜?\d?)\(?([^)]*)\)?', jo_text)
                title_id = match.group(1) if match else jo_text[:10]
                
                parts = [jo_text]
                
                hang_list = self._force_list(jo.get('í•­', []))
                for hang in hang_list:
                    h_content = self._clean_html(hang.get('í•­ë‚´ìš©', ''))
                    if h_content: parts.append(f"  {h_content}")
                    
                    ho_list = self._force_list(hang.get('í˜¸', []))
                    for ho in ho_list:
                        ho_content = self._clean_html(ho.get('í˜¸ë‚´ìš©', ''))
                        if ho_content: parts.append(f"    {ho_content}")
                        
                        mok_list = self._force_list(ho.get('ëª©', []))
                        for mok in mok_list:
                            m_content = self._clean_html(mok.get('ëª©ë‚´ìš©', ''))
                            if m_content: parts.append(f"      {m_content}")
                
                full_text = "\n".join(parts)
                articles.append({'id': title_id, 'content': full_text})

            # (2) ë³„í‘œ íŒŒì‹±
            byeol_root = root.get('ë³„í‘œ', {})
            if isinstance(byeol_root, list):
                byeol_list = byeol_root
            else:
                byeol_list = self._force_list(byeol_root.get('ë³„í‘œë‹¨ìœ„', []))
            
            for b in byeol_list:
                b_title = self._clean_html(b.get('ë³„í‘œì œëª©', 'ë³„í‘œ'))
                b_content = self._clean_html(b.get('ë³„í‘œë‚´ìš©', ''))
                if not b_content:
                    link = b.get('ë³„í‘œì„œì‹íŒŒì¼ë§í¬') or b.get('ë³„í‘œì„œì‹PDFíŒŒì¼ë§í¬')
                    if link: b_content = f"[ì„œì‹ íŒŒì¼ ì¡´ì¬] {link}"
                
                if b_content or b_title:
                    articles.append({'id': f"[ë³„í‘œ] {b_title}", 'content': b_content or "ë‚´ìš© ì—†ìŒ"})

        # 2. í–‰ì •ê·œì¹™ (AdmRul)
        elif 'í–‰ì •ê·œì¹™' in data:
            root = data['í–‰ì •ê·œì¹™']
            jo_list = self._force_list(root.get('ì¡°ë¬¸', {}).get('ì¡°ë¬¸ë‹¨ìœ„', []))
            if jo_list:
                for jo in jo_list:
                    jo_content = self._clean_html(jo.get('ì¡°ë¬¸ë‚´ìš©', ''))
                    
                    match = re.match(r'(ì œ\d+ì¡°(?:ì˜\d+)?)\(?([^)]*)\)?', jo_content)
                    if match:
                        title_id = f"{match.group(1)}{' ' + match.group(2) if match.group(2) else ''}"
                    else:
                        match_num = re.match(r'^(\d+\.|[ê°€-í£]\.)\s*(.*)', jo_content)
                        if match_num:
                            title_id = f"{match_num.group(1)} {match_num.group(2)[:10]}..."
                        else:
                            title_id = jo_content[:20].strip() or "ì¡°ë¬¸"

                    parts = [jo_content]
                    hang_list = self._force_list(jo.get('í•­', []))
                    for hang in hang_list:
                        h = self._clean_html(hang.get('í•­ë‚´ìš©', ''))
                        if h: parts.append(f"  {h}")
                        
                        ho_list = self._force_list(hang.get('í˜¸', []))
                        for ho in ho_list:
                            ho_content = self._clean_html(ho.get('í˜¸ë‚´ìš©', ''))
                            if ho_content:
                                parts.append(f"    {ho_content}")
                    
                    articles.append({'id': title_id, 'content': "\n".join(parts)})
            
            # ë³„í‘œ
            byeol_root = root.get('ë³„í‘œ', {})
            if isinstance(byeol_root, list):
                byeol_list = byeol_root
            else:
                byeol_list = self._force_list(byeol_root.get('ë³„í‘œë‹¨ìœ„', []))
                
            for b in byeol_list:
                b_title = self._clean_html(b.get('ë³„í‘œì œëª©', 'ë³„í‘œ'))
                b_content = self._clean_html(b.get('ë³„í‘œë‚´ìš©', ''))
                if not b_content:
                    link = b.get('ë³„í‘œì„œì‹íŒŒì¼ë§í¬') or b.get('ë³„í‘œì„œì‹PDFíŒŒì¼ë§í¬')
                    if link: b_content = f"[ì„œì‹ íŒŒì¼ ì¡´ì¬] {link}"

                if b_content or b_title:
                    articles.append({'id': f"[ë³„í‘œ] {b_title}", 'content': b_content or "ë‚´ìš© ì—†ìŒ"})

        # 3. íŒë¡€ (Prec)
        elif 'íŒë¡€' in data:
            root = data['íŒë¡€']
            issue = self._clean_html(root.get('íŒì‹œì‚¬í•­', ''))
            if issue:
                articles.append({'id': 'íŒì‹œì‚¬í•­', 'content': issue})
            
            summary = self._clean_html(root.get('íŒê²°ìš”ì§€', ''))
            if summary:
                articles.append({'id': 'íŒê²°ìš”ì§€', 'content': summary})
            else:
                content = self._clean_html(root.get('íŒë¡€ë‚´ìš©', ''))
                if content:
                    articles.append({'id': 'íŒë¡€ë‚´ìš©', 'content': content[:3000] + "...(ìƒëµ)"})
        
        return articles

        return self._parse_xml_to_text(data), view_url, data

    async def ai_search(self, query: str, search_scope: int = 0) -> List[Dict[str, Any]]:
        """
        Intelligent Law Search (aiSearch) - í†µí•© ë²•ë ¹/ê·œì¹™ ê²€ìƒ‰
        search_scope: 0=ë²•ë ¹ì¡°ë¬¸, 1=ë²•ë ¹ë³„í‘œ/ì„œì‹, 2=í–‰ì •ê·œì¹™ì¡°ë¬¸, 3=í–‰ì •ê·œì¹™ë³„í‘œ
        Returns: List of dicts with 'law_name', 'article_title', 'content', 'link'
        """
        if self.is_mock: return []

        endpoint = "lawSearch.do"
        params = {
            "OC": self.api_id,
            "target": "aiSearch",
            "type": "XML",
            "search": search_scope,
            "query": query,
            "display": 20, 
            "page": 1
        }
        
        # print(f"      ğŸ“¡ [AI Search] Query='{query}' | Scope={search_scope}")
        
        query_string = urlencode(params, doseq=True)
        url = f"{self.base_url}/DRF/{endpoint}?{query_string}"
        
        data = await self._fetch(url)
        
        results = []
        try:
            root = data.get('aiSearch', {})
            # aiSearch results are usually under <ë²•ë ¹ì¡°ë¬¸> for scope 0, let's check for others dynamically
            # XML parsing (xmltodict) puts repeating elements in a list, or a single dict if only one.
            # We look for common result keys.
            
            candidates = []
            
            # Case 0: Law Articles
            if 'ë²•ë ¹ì¡°ë¬¸' in root:
                candidates.extend(self._force_list(root['ë²•ë ¹ì¡°ë¬¸']))
            # Case 2: AdmRul Articles (Guessing key name, usually matches the item type)
            # If the API unifies them under 'ë²•ë ¹ì¡°ë¬¸' even for scope 2, we are good. 
            # If not, checking typical AdmRul keys.
            if 'í–‰ì •ê·œì¹™ì¡°ë¬¸' in root:
                candidates.extend(self._force_list(root['í–‰ì •ê·œì¹™ì¡°ë¬¸']))
                
            for item in candidates:
                # Extract fields
                # Common fields observed: <ë²•ë ¹ëª…>, <ì¡°ë¬¸ì œëª©>, <ì¡°ë¬¸ë‚´ìš©>
                
                # Law Name
                law_name = item.get('ë²•ë ¹ëª…') or item.get('í–‰ì •ê·œì¹™ëª…') 
                if isinstance(law_name, dict): law_name = law_name.get('#text', '') # CDATA handling if simple dict

                # Article Title/Num
                art_num = item.get('ì¡°ë¬¸ë²ˆí˜¸', '?')
                art_title = item.get('ì¡°ë¬¸ì œëª©')
                if isinstance(art_title, dict): art_title = art_title.get('#text', '')
                
                # Content
                content_raw = item.get('ì¡°ë¬¸ë‚´ìš©', '')
                if isinstance(content_raw, dict): content_raw = content_raw.get('#text', '')
                content = self._clean_html(content_raw)
                
                # Link construction (Standard View Link)
                # aiSearch items have <ë²•ë ¹ì¼ë ¨ë²ˆí˜¸>, <ì¡°ë¬¸ì¼ë ¨ë²ˆí˜¸> etc.
                # Standard link: http://www.law.go.kr/DRF/lawService.do?target=law&OC={id}&ID={law_id}&type=HTML...
                # Simpler to just use what we have or generic link
                # Found <ë²•ë ¹ID> in result.
                law_id = item.get('ë²•ë ¹ID', '')
                link = f"http://www.law.go.kr/LSW/lsInfoP.do?lsiSeq={item.get('ë²•ë ¹ì¼ë ¨ë²ˆí˜¸', '')}"
                
                results.append({
                    'law_name': str(law_name),
                    'article_title': f"ì œ{art_num}ì¡°({str(art_title)})",
                    'content': content,
                    'link': link,
                    'raw': item
                })
                
            print(f"      ğŸ¤– [AI Search] '{query}' (Scope {search_scope}) -> {len(results)}ê±´ ë°œê²¬")
            return results
            
        except Exception as e:
            print(f"      âš ï¸ AI Search Parse Error: {e}")
            return []


# Global instance
law_api = NationalLawAPI(api_id="jaeyeongm34")
print("âœ… NationalLawAPI Initialized (with 'eflaw' support).")
