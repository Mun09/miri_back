import os
import json
import asyncio
import aiohttp
import xmltodict
import json_repair
import re
from dotenv import load_dotenv
from urllib.parse import urlencode, urlparse, parse_qs, urlunparse
from typing import Any, List, Dict, Tuple, Optional
from pydantic import BaseModel, Field

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

MAX_ANALYSIS_DOCS = 5

try:
    from openai import AsyncOpenAI
except ImportError:
    raise ImportError("Please run 'pip install openai' to use this feature.")

# 4. Define LLM Client (OpenAI - Cost Optimized)
class OpenAIClient:
    def __init__(self):
        # API KeyëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œí•˜ê±°ë‚˜ ì—¬ê¸°ì— ì§ì ‘ ì…ë ¥
        self.api_key = os.getenv("OPENAI_API_KEY") 
        if not self.api_key:
            print("âš ï¸ Warning: OPENAI_API_KEY not found in environment variables.")
        
        self.client = AsyncOpenAI(api_key=self.api_key)
        self.semaphore = asyncio.Semaphore(5) # LLM ë™ì‹œ ìš”ì²­ ì œí•œ (Rate Limit ë°©ì§€)

    async def generate(self, system_prompt: str, user_input: str, model: str = "gpt-4o-mini", **kwargs) -> str:
        async with self.semaphore:
            try:
                response = await self.client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_input}
                    ],
                    temperature=kwargs.get("temperature", 0.7),
                    max_tokens=kwargs.get("max_tokens", 2048)
                )
                return response.choices[0].message.content.strip()
            except Exception as e:
                print(f"LLM Error ({model}): {e}")
                return "{}" # Return empty JSON-like string on error to prevent json parsing crash

llm_client = OpenAIClient()

import asyncio
import aiohttp
import xmltodict
from urllib.parse import urlencode, urlparse, parse_qs, urlunparse
from typing import Any, List, Dict, Tuple, Optional
from pydantic import BaseModel, Field
import json_repair
import json

class NationalLawAPI:
    def __init__(self, api_id="jaeyeongm34", base_url="http://www.law.go.kr"):
        self.base_url = base_url
        self.api_id = api_id
        self.is_mock = not bool(self.api_id)
        self._cache = {}
        self.semaphore = asyncio.Semaphore(20) # ë™ì‹œ ìš”ì²­ ì œí•œ

    def _force_list(self, data: Any) -> List[Any]:
        if not data: return []
        if isinstance(data, list): return data
        return [data]

    async def _fetch(self, url: str) -> Dict[str, Any]:
        if url in self._cache:
            # print(f"      ğŸ“¦ Cache Hit: {url}")
            return self._cache[url]
        
        async with self.semaphore:
            async with aiohttp.ClientSession() as session:
                try:
                    async with session.get(url) as response:
                        if response.status != 200:
                            print(f"    âŒ API Error: Status {response.status} for {url}")
                            return {}
                        content = await response.text()
                        parsed = xmltodict.parse(content)
                        self._cache[url] = parsed
                        return parsed
                except Exception as e:
                    print(f"    âŒ Fetch Exception: {e}")
                    return {}

    async def search_list(self, target: str, query: str, **kwargs) -> List[Dict[str, Any]]:
        if self.is_mock: return []

        endpoint = "lawSearch.do"
        params = {
            "OC": self.api_id,
            "target": target,
            "type": "XML",
            "query": query,
            "display": 5,
            "nw": 3  # ê¸°ë³¸ê°’: í˜„í–‰ ë²•ë ¹ë§Œ ê²€ìƒ‰
        }
        params.update(kwargs) # JO ë“± ì¶”ê°€ íŒŒë¼ë¯¸í„° ë³‘í•©

        # ë””ë²„ê¹…: ì‹¤ì œ ìš”ì²­ URL íŒŒë¼ë¯¸í„° ì¶œë ¥ (íŒë¡€ ê²€ìƒ‰ì‹œ ì¤‘ìš”)
        if target == 'prec':
            jo_param = kwargs.get('JO', '')
            print(f"      ğŸ“¡ [API ìš”ì²­] {target.upper()} ê²€ìƒ‰ | Query='{query}' | JO='{jo_param}'")
        else:
            print(f"      ğŸ“¡ [API ìš”ì²­] {target.upper()} ê²€ìƒ‰ | Query='{query}'")

        query_string = urlencode(params, doseq=True)
        url = f"{self.base_url}/DRF/{endpoint}?{query_string}"

        data = await self._fetch(url)

        # [Update] eflaw ì§€ì› ì¶”ê°€ (eflawì˜ rootëŠ” LawSearch)
        root_map = {'law': 'LawSearch', 'eflaw': 'LawSearch', 'admrul': 'AdmRulSearch', 'prec': 'PrecSearch'}
        root_key = root_map.get(target, 'LawSearch')

        try:
            search_data = data.get(root_key, {})
            # [Fix] eflaw ê²€ìƒ‰ ê²°ê³¼ì˜ ì•„ì´í…œ íƒœê·¸ëŠ” 'law'ì„
            item_key = 'law' if target == 'eflaw' else target

            result = search_data.get(item_key, [])
            items = self._force_list(result)
            print(f"        -> âœ… ê²°ê³¼: {len(items)}ê±´ ë°œê²¬")
            return items
        except AttributeError:
            print(f"        -> âš ï¸ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨ ë˜ëŠ” 0ê±´")
            return []

    def _clean_html(self, text: str) -> str:
        """HTML íƒœê·¸ ì œê±° + ë¶ˆí•„ìš”í•œ ë©”íƒ€ë°ì´í„° ì •ë¦¬"""
        if not text: return ""
        
        # 1. HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text).strip()
        
        # 2. ê°œì • ì´ë ¥ ì œê±° (ì˜ˆ: <ê°œì • 2009.1.30>, <ì‹ ì„¤ 2017.1.17>)
        text = re.sub(r'<(ê°œì •|ì‹ ì„¤|ì „ë¬¸ê°œì •|íƒ€ë²•ê°œì •|ì¼ë¶€ê°œì •|íì§€)\s+[\d.,\s]+>', '', text)
        
        # 3. ì°¸ê³  ì •ë³´ ì œê±° (ì˜ˆ: [ì „ë¬¸ê°œì • 2009.1.30])
        text = re.sub(r'\[(ì „ë¬¸ê°œì •|ê°œì •|ì‹ ì„¤|íƒ€ë²•ê°œì •|ì¼ë¶€ê°œì •|íì§€)\s+[\d.,\s]+\]', '', text)
        
        # 4. ì¥/ì ˆ/ê´€ í—¤ë” ì œê±° (ì˜ˆ: "ì œ1ì¥ ì´ì¹™", "ì œ2ì ˆ ì™¸êµ­í™˜ì—…ë¬´")
        text = re.sub(r'ì œ\d+ì¥\s+[ê°€-í£\s]+', '', text)
        text = re.sub(r'ì œ\d+ì ˆ\s+[ê°€-í£\s]+', '', text)
        
        # 5. ë‹¤ì¤‘ ê³µë°±/ê°œí–‰ ì •ë¦¬
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text

    def _parse_xml_to_text(self, data: Dict[str, Any]) -> str:
        # 1. ë²•ë ¹ (Law)
        if 'ë²•ë ¹' in data:
            root = data['ë²•ë ¹']
            text_parts = []
            
            # ê¸°ë³¸ í˜•ì‹: ë²•ë ¹ëª…
            title = root.get('ê¸°ë³¸ì •ë³´', {}).get('ë²•ë ¹ëª…_í•œê¸€', '')
            text_parts.append(f"== {title} ==\n")

            # ì¡°ë¬¸ (Main Body)
            jo_list = self._force_list(root.get('ì¡°ë¬¸', {}).get('ì¡°ë¬¸ë‹¨ìœ„', []))
            for jo in jo_list:
                # ì¡°ë¬¸ë‚´ìš© (ì œXì¡° ...)
                jo_content = self._clean_html(jo.get('ì¡°ë¬¸ë‚´ìš©', ''))
                item_text = [jo_content]
                
                # í•­ (Paragraph)
                hang_list = self._force_list(jo.get('í•­', []))
                for hang in hang_list:
                    h_content = self._clean_html(hang.get('í•­ë‚´ìš©', ''))
                    if h_content:
                        item_text.append(f"  {h_content}")
                    
                    # í˜¸ (Subparagraph)
                    ho_list = self._force_list(hang.get('í˜¸', []))
                    for ho in ho_list:
                        ho_content = self._clean_html(ho.get('í˜¸ë‚´ìš©', ''))
                        if ho_content:
                            item_text.append(f"    {ho_content}")
                
                text_parts.append("\n".join(item_text))
            
            return "\n\n".join(text_parts) if len(text_parts) > 1 else str(data)

        # 2. í–‰ì •ê·œì¹™ (AdmRul)
        if 'í–‰ì •ê·œì¹™' in data:
            root = data['í–‰ì •ê·œì¹™']
            text_parts = []
            
            title = root.get('ê¸°ë³¸ì •ë³´', {}).get('í–‰ì •ê·œì¹™ëª…', '')
            text_parts.append(f"== {title} ==\n")
            
            # ì¡°ë¬¸ êµ¬ì¡°ê°€ ìˆëŠ” ê²½ìš°
            jo_list = self._force_list(root.get('ì¡°ë¬¸', {}).get('ì¡°ë¬¸ë‹¨ìœ„', []))
            if jo_list:
                for jo in jo_list:
                    jo_content = self._clean_html(jo.get('ì¡°ë¬¸ë‚´ìš©', ''))
                    text_parts.append(jo_content)
                    
                    # í•­/í˜¸ ì²˜ë¦¬ (í–‰ì •ê·œì¹™ì€ êµ¬ì¡°ê°€ ëœ ì—„ê²©í•  ìˆ˜ ìˆìŒ)
                    hang_list = self._force_list(jo.get('í•­', []))
                    for hang in hang_list:
                        h_content = self._clean_html(hang.get('í•­ë‚´ìš©', ''))
                        if h_content: text_parts.append(f"  {h_content}")
            else:
                 # ì¡°ë¬¸ í˜•ì‹ì´ ì•„ë‹Œ ë³¸ë¬¸ í†µì§œì¸ ê²½ìš°
                 text_parts.append(self._clean_html(root.get('ë³¸ë¬¸', '')))

            return "\n\n".join(text_parts)

        # 3. íŒë¡€ (Prec)
        if 'íŒë¡€' in data:
            root = data['íŒë¡€']
            issue = self._clean_html(root.get('íŒì‹œì‚¬í•­', ''))
            summary = self._clean_html(root.get('íŒê²°ìš”ì§€', ''))
            content = self._clean_html(root.get('íŒë¡€ë‚´ìš©', ''))
            
            return f"[íŒì‹œì‚¬í•­]\n{issue}\n\n[íŒê²°ìš”ì§€]\n{summary}\n\n[íŒë¡€ë‚´ìš©]\n{content}"

        return str(data)

    def _get_unique_id(self, data: Dict[str, Any]) -> str:
        """XML ë°ì´í„°ì—ì„œ ê³ ìœ  ì‹ë³„ì ì¶”ì¶œ"""
        if 'ë²•ë ¹' in data:
            return str(data['ë²•ë ¹'].get('ê¸°ë³¸ì •ë³´', {}).get('ë²•ë ¹ID', 'Unknown'))
        if 'í–‰ì •ê·œì¹™' in data:
            return str(data['í–‰ì •ê·œì¹™'].get('ê¸°ë³¸ì •ë³´', {}).get('í–‰ì •ê·œì¹™ì¼ë ¨ë²ˆí˜¸', 'Unknown'))
        if 'íŒë¡€' in data:
            return str(data['íŒë¡€'].get('íŒë¡€ì •ë³´ì¼ë ¨ë²ˆí˜¸', 'Unknown'))
        return "Unknown"

    def _parse_law_structure(self, data: Dict[str, Any]) -> List[Dict[str, str]]:
        """XML êµ¬ì¡°ë¥¼ í™œìš©í•´ ë²•ë ¹ì˜ [ì¡°ë¬¸/ë³„í‘œ] ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ (í•­/í˜¸/ëª© ê³„ì¸µ ì™„ë²½ ëŒ€ì‘)"""
        articles = []
        
        # 1. ë²•ë ¹ (Law)
        if 'ë²•ë ¹' in data:
            root = data['ë²•ë ¹']
            
            # (1) ì¡°ë¬¸ íŒŒì‹±
            jo_list = self._force_list(root.get('ì¡°ë¬¸', {}).get('ì¡°ë¬¸ë‹¨ìœ„', []))
            for jo in jo_list:
                # [OPTIMIZATION] ì „ë¬¸(ç« ç¯€ í—¤ë”)ì€ ìŠ¤í‚µ (ì‹¤ì œ ê·œì • ì•„ë‹˜)
                jo_type = jo.get('ì¡°ë¬¸ì—¬ë¶€', '')
                if jo_type == 'ì „ë¬¸':
                    continue
                
                # ì¡°ë¬¸ ì œëª© (ì œ1ì¡°(ëª©ì ))
                jo_text = self._clean_html(jo.get('ì¡°ë¬¸ë‚´ìš©', ''))
                
                # ë¹ˆ ì¡°ë¬¸ ìŠ¤í‚µ (ì‚­ì œëœ ì¡°í•­ ë“±)
                if not jo_text or len(jo_text) < 5:
                    continue
                
                match = re.match(r'(ì œ\d+ì¡°ì˜?\d?)\(?([^)]*)\)?', jo_text)
                title_id = match.group(1) if match else jo_text[:10]
                
                parts = [jo_text]
                
                # í•­ (Paragraph)
                hang_list = self._force_list(jo.get('í•­', []))
                for hang in hang_list:
                    h_content = self._clean_html(hang.get('í•­ë‚´ìš©', ''))
                    if h_content: parts.append(f"  {h_content}")
                    
                    # í˜¸ (Subparagraph)
                    ho_list = self._force_list(hang.get('í˜¸', []))
                    for ho in ho_list:
                        ho_content = self._clean_html(ho.get('í˜¸ë‚´ìš©', ''))
                        if ho_content: parts.append(f"    {ho_content}")
                        
                        # ëª© (Item)
                        mok_list = self._force_list(ho.get('ëª©', []))
                        for mok in mok_list:
                             m_content = self._clean_html(mok.get('ëª©ë‚´ìš©', ''))
                             if m_content: parts.append(f"      {m_content}")
                
                full_text = "\n".join(parts)
                articles.append({'id': title_id, 'content': full_text})

            # (2) ë³„í‘œ íŒŒì‹± (ì¤‘ìš”)
            # êµ¬ì¡°: <ë³„í‘œ> -> <ë³„í‘œë‹¨ìœ„> ë¦¬ìŠ¤íŠ¸ ê°€ëŠ¥ì„±
            byeol_root = root.get('ë³„í‘œ', {})
            # ë§Œì•½ ë³„í‘œê°€ listë¼ë©´ ë°”ë¡œ ì‚¬ìš© (.get('ë³„í‘œ')ê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°)
            if isinstance(byeol_root, list):
                byeol_list = byeol_root
            else:
                # ë”•ì…”ë„ˆë¦¬ë¼ë©´ 'ë³„í‘œë‹¨ìœ„' í™•ì¸
                byeol_list = self._force_list(byeol_root.get('ë³„í‘œë‹¨ìœ„', []))
            
            for b in byeol_list:
                b_title = self._clean_html(b.get('ë³„í‘œì œëª©', 'ë³„í‘œ'))
                # ë³„í‘œë‚´ìš©ì´ ì—†ìœ¼ë©´ 'íŒŒì¼ë§í¬'ë¼ë„ í™•ì¸
                b_content = self._clean_html(b.get('ë³„í‘œë‚´ìš©', ''))
                if not b_content:
                     # ë‚´ìš©ì´ ë¹„ì–´ìˆë‹¤ë©´, ì„œì‹ íŒŒì¼ ë§í¬ê°€ ìˆëŠ”ì§€ í™•ì¸í•´ì„œ ì•ˆë‚´
                     link = b.get('ë³„í‘œì„œì‹íŒŒì¼ë§í¬') or b.get('ë³„í‘œì„œì‹PDFíŒŒì¼ë§í¬')
                     if link: b_content = f"[ì„œì‹ íŒŒì¼ ì¡´ì¬] {link}"
                
                if b_content or b_title: # ì œëª©ì´ë¼ë„ ìˆìœ¼ë©´ ì¶”ê°€
                    articles.append({'id': f"[ë³„í‘œ] {b_title}", 'content': b_content or "ë‚´ìš© ì—†ìŒ (ì„œì‹ íŒŒì¼ í™•ì¸ í•„ìš”)"})

        # 2. í–‰ì •ê·œì¹™ (AdmRul)
        elif 'í–‰ì •ê·œì¹™' in data:
            root = data['í–‰ì •ê·œì¹™']
            # ì¡°ë¬¸
            jo_list = self._force_list(root.get('ì¡°ë¬¸', {}).get('ì¡°ë¬¸ë‹¨ìœ„', []))
            if jo_list:
                for jo in jo_list:
                    jo_content = self._clean_html(jo.get('ì¡°ë¬¸ë‚´ìš©', ''))
                    # í•­/í˜¸ê°€ ìˆì„ ìˆ˜ ìˆìŒ
                    parts = [jo_content]
                    hang_list = self._force_list(jo.get('í•­', []))
                    for hang in hang_list:
                        h = self._clean_html(hang.get('í•­ë‚´ìš©', ''))
                        if h: parts.append(f"  {h}")
                    
                    articles.append({'id': jo_content[:20], 'content': "\n".join(parts)})
            
            # ë³„í‘œ
            byeol_root = root.get('ë³„í‘œ', {})
            if isinstance(byeol_root, list):
                byeol_list = byeol_root
            else:
                byeol_list = self._force_list(byeol_root.get('ë³„í‘œë‹¨ìœ„', []))
                
            for b in byeol_list:
                b_title = self._clean_html(b.get('ë³„í‘œì œëª©', 'ë³„í‘œ'))
                b_content = self._clean_html(b.get('ë³„í‘œë‚´ìš©', ''))
                if not b_content:
                     link = b.get('ë³„í‘œì„œì‹íŒŒì¼ë§í¬') or b.get('ë³„í‘œì„œì‹PDFíŒŒì¼ë§í¬')
                     if link: b_content = f"[ì„œì‹ íŒŒì¼ ì¡´ì¬] {link}"

                if b_content or b_title:
                    articles.append({'id': f"[ë³„í‘œ] {b_title}", 'content': b_content or "ë‚´ìš© ì—†ìŒ"})

        # 3. íŒë¡€ (Prec) - êµ¬ì¡° ë¶„ì„
        elif 'íŒë¡€' in data:
            root = data['íŒë¡€']
            # íŒì‹œì‚¬í•­ (Issues)
            issue = self._clean_html(root.get('íŒì‹œì‚¬í•­', ''))
            if issue:
                articles.append({'id': 'íŒì‹œì‚¬í•­', 'content': issue})
            
            # íŒê²°ìš”ì§€ (Summary)
            summary = self._clean_html(root.get('íŒê²°ìš”ì§€', ''))
            if summary:
                articles.append({'id': 'íŒê²°ìš”ì§€', 'content': summary})
            else:
                 # ìš”ì§€ê°€ ì—†ëŠ” ê²½ìš° ë³¸ë¬¸ ì‚¬ìš© (Fallback)
                 content = self._clean_html(root.get('íŒë¡€ë‚´ìš©', ''))
                 if content:
                     articles.append({'id': 'íŒë¡€ë‚´ìš©', 'content': content[:3000] + "...(ìƒëµ)"})
        
        return articles

    async def get_content_from_item(self, item: Dict[str, Any]) -> Tuple[str, str, Any]:
        """Returns: (Text, URL, RawDataDict)"""
        if self.is_mock: return "Mock Content", "", {}
        link = item.get('ë²•ë ¹ìƒì„¸ë§í¬') or item.get('í–‰ì •ê·œì¹™ìƒì„¸ë§í¬') or item.get('íŒë¡€ìƒì„¸ë§í¬')
        if not link: return "", "", {}

        full_url = f"{self.base_url}{link}"
        parsed = urlparse(full_url)
        query_params = parse_qs(parsed.query)
        query_params['type'] = ['XML'] # XML ê°•ì œ
        new_query = urlencode(query_params, doseq=True)
        final_url = urlunparse(parsed._replace(query=new_query))

        data = await self._fetch(final_url)
        
        # [Update] XML êµ¬ì¡°í™” íŒŒì‹± ì ìš© + URL ë°˜í™˜ + ì›ë³¸ ë°ì´í„° ë°˜í™˜ (êµ¬ì¡° í™œìš© ìœ„í•´)
        view_url = final_url.replace("type=XML", "type=HTML") 
        return self._parse_xml_to_text(data), view_url, data

law_api = NationalLawAPI(api_id="jaeyeongm34")
print("âœ… NationalLawAPI Updated (with 'eflaw' support).")

# 6. Define Module Schemas

class Stakeholders(BaseModel):
    platform_role: str = Field(description="Role of the platform")
    users: List[str] = Field(description="List of user types")

class Mechanisms(BaseModel):
    money_flow: str = Field(description="How money moves")
    data_collection: List[str] = Field(description="What data is collected")
    service_delivery: str = Field(description="How service is delivered")

class BusinessModel(BaseModel):
    project_name: str = Field(description="Name of the project")
    business_type: str = Field(description="Type of business")
    stakeholders: Stakeholders
    mechanisms: Mechanisms
    regulatory_tags: List[str] = Field(description="List of regulatory keywords")

class AtomicAction(BaseModel):
    actor: str
    action: str
    object: str

class Scenario(BaseModel):
    name: str
    type: str
    actions: List[AtomicAction]

class LegalEvidence(BaseModel):
    relevant_laws: List[str] = Field(default_factory=list)
    summary: str

class RiskReport(BaseModel):
    verdict: str = Field(description="Status: Safe | Caution | Danger | Review Required")
    summary: str = Field(description="Detailed judgement summary")
    key_issues: List[str] = Field(default_factory=list, description="List of key legal issues")
    citation: str = Field(default="", description="Relevant laws")

class DocumentReview(BaseModel):
    law_name: str
    key_clause: str = Field(description="ê´€ë ¨ ì¡°í•­ (ì˜ˆ: ì œ3ì¡° ì œ1í•­). ì—†ìœ¼ë©´ ë¹ˆì¹¸")
    status: str = Field(description="Prohibited | Permitted | Conditional | Neutral | Ambiguous")
    summary: str = Field(description="í•´ë‹¹ ì¡°í•­ì˜ í•µì‹¬ ë‚´ìš© ìš”ì•½ (í•œê¸€ 2ë¬¸ì¥ ì´ë‚´)")
    url: str = Field(description="ë²•ë ¹/íŒë¡€ ì›ë¬¸ ë§í¬", default="")

print("Classes defined.")

class Structurer:
    SYSTEM_PROMPT = """
    You are an expert 'Business Model Structurer'.
    Analyze the user's idea and structure it into a formal business model.
    
    Output MUST be a JSON object following this schema:
    {
        "project_name": "...",
        "business_type": "...",
        "stakeholders": {
            "platform_role": "...",
            "users": ["..."]
        },
        "mechanisms": {
            "money_flow": "...",
            "data_collection": ["..."],
            "service_delivery": "..."
        },
        "regulatory_tags": ["..."]
    }
    """
    async def execute(self, user_input: str) -> BusinessModel:
        print("\n[1] Structuring Business Model...")
        # [MODEL: GPT-4o] ì •í™•í•œ êµ¬ì¡°í™”ê°€ ê°€ì¥ ì¤‘ìš”í•¨ (Initial Input)
        response = await llm_client.generate(self.SYSTEM_PROMPT, user_input, model="gpt-4o")
        try:
            return BusinessModel(**json_repair.loads(response))
        except Exception as e:
            print(f"Structurer Error: {e}")
            print(f"Raw: {response}")
            raise e
# [ìˆ˜ì •] 1. êµ¬ì²´ì ì¸ ë””í…Œì¼ì„ ìƒì„±í•˜ë„ë¡ Simulator ê°•í™”
class Simulator:
    SYSTEM_PROMPT = """
    You are a 'Regulatory Sandbox Simulator'.
    Based on the Business Model, generate ONE representative 'Main Scenario'.

    [Important] To enable legal judgment, you MUST include 'specific figures' and 'clear actions'.

    Output MUST be a JSON list adhering to this structure:
    [
        {
            "name": "Main Scenario Summary",
            "type": "Main",
            "actions": [
                {"actor": "Traveler A", "action": "Registers remaining $2,000 on platform (90% exchange rate preference)", "object": "$2,000 USD"},
                {"actor": "Buyer B", "action": "Meets in person to exchange cash after chatting in app", "object": "Cash"}
            ]
        }
    ]
    """

    async def execute(self, model: BusinessModel) -> List[Scenario]:
        print("\n[2] Simulating Scenarios (Single Representative)...")
        prompt = f"Business Model: {model.model_dump_json()}"
        # [MODEL: GPT-4o-mini] ì°½ì˜ì ì¸ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±ì€ minië„ ì¶©ë¶„íˆ ì˜í•¨ (ë¹„ìš© ì ˆê°)
        response = await llm_client.generate(self.SYSTEM_PROMPT, prompt, model="gpt-4o-mini")
        try:
            data = json_repair.loads(response)
            if isinstance(data, dict): data = [data]
            return [Scenario(**s) for s in data][:1] # ê°•ì œë¡œ 1ê°œë§Œ ì„ íƒ
        except Exception as e:
            print(f"Simulator Error: {e}")
            return []

# [ìˆ˜ì •] 3. ì—„ê²©í•œ ë²•ë¥ ê°€ í˜ë¥´ì†Œë‚˜ ì ìš©
class Auditor:
    SYSTEM_PROMPT = """
    ë‹¹ì‹ ì€ ê¹ê¹í•œ 'ê¸ˆìœµ ê·œì œ ê°ì‚¬ê´€(Compliance Officer)'ì…ë‹ˆë‹¤.
    ì œê³µëœ [ì‹œë‚˜ë¦¬ì˜¤]ì˜ êµ¬ì²´ì  í–‰ìœ„ê°€ [ìˆ˜ì§‘ëœ ë²•ì  ê·¼ê±°]ì— ìœ„ë°°ë˜ëŠ”ì§€ ì—„ê²©í•˜ê²Œ íŒë‹¨í•˜ì‹­ì‹œì˜¤.

    [íŒë‹¨ ê¸°ì¤€]
    1. ë²•ì  ê·¼ê±°ì— 'ê¸ˆì§€', 'í—ˆê°€ í•„ìš”', 'ë“±ë¡ ì˜ë¬´' ë“±ì˜ ë‹¨ì–´ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì‹­ì‹œì˜¤.
    2. ê¸ˆì•¡(ì˜ˆ: ë¯¸í™” 5,000ë‹¬ëŸ¬ ë“±)ì´ ë²•ì  í•œë„ë¥¼ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸í•˜ì‹­ì‹œì˜¤.
    3. êµ¬ì²´ì ì¸ ì¡°í•­(ì œXì¡°)ì´ ì¸ìš©ë˜ì§€ ì•Šì•˜ë‹¤ë©´ 'Unknown'ìœ¼ë¡œ ì²˜ë¦¬í•˜ì§€ ë§ê³ , "ê·œì œ ê³µë°± ê°€ëŠ¥ì„±"ìœ¼ë¡œ ê²½ê³ (Warning)í•˜ì‹­ì‹œì˜¤.

    Output JSON Format:
    {
        "risk_level": "Critical | Warning | Safe",
        "verdict": "íŒë‹¨ ì´ìœ  (ë°˜ë“œì‹œ ìˆ˜ì§‘ëœ ë²•ë ¹ì˜ ì¡°í•­ì„ ì¸ìš©í•˜ì—¬ ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª…)",
        "citation": "ìœ„ë°˜ë˜ê±°ë‚˜ ê²€í† ê°€ í•„ìš”í•œ êµ¬ì²´ì  ë²•ë ¹ëª… ë° ì¡°í•­ (ì˜ˆ: ì™¸êµ­í™˜ê±°ë˜ë²• ì œ8ì¡°)"
    }
    ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
    """

    async def execute(self, scenario: Scenario, evidence: LegalEvidence) -> RiskReport:
        evidence_text = "\n".join(evidence.relevant_laws)
        if not evidence_text: evidence_text = "ê´€ë ¨ëœ êµ¬ì²´ì  ë²•ë ¹ì„ ë°œê²¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

        prompt = f"""
        [ì‹œë‚˜ë¦¬ì˜¤]
        {scenario.model_dump_json()}

        [ìˆ˜ì§‘ëœ ë²•ì  ê·¼ê±°]
        {evidence_text}
        """
        response = await llm_client.generate(self.SYSTEM_PROMPT, prompt)
        try:
            data = json_repair.loads(response)
            return RiskReport(**data)
        except Exception as e:
            print(f"Audit Error: {e}")
            return RiskReport(risk_level="Unknown", verdict="Audit Failed", citation="")

# ì¸ìŠ¤í„´ìŠ¤ ì¬ìƒì„±
structurer = Structurer()
simulator = Simulator()
auditor = Auditor()

import re
from typing import List, Dict, Tuple, Optional, Callable

class SearchStrategy(BaseModel):
    rationale: str = Field(description="ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½ ì´ìœ ")
    databases: List[str] = Field(description="ê²€ìƒ‰í•  DB ëª©ë¡ (ìˆœì„œëŒ€ë¡œ ì¤‘ìš”)", default=["law", "admrul"])
    focus_keywords: List[str] = Field(description="ì „ëµì ìœ¼ë¡œ ì§‘ì¤‘í•  ì¶”ê°€ í‚¤ì›Œë“œ", default_factory=list)

class Investigator:
    """
    Expert Investigator with Self-Correction (Critic) & Action-Based Search & Strategic Planning
    """
    
    STRATEGY_PROMPT = """
    Analyze the legal nature of the following action to decide the search strategy.

    [Action]
    {action}

    [Database Characteristics]
    - law (Acts, Decrees): For clear prohibitions, permissions, and penalties.
    - admrul (Administrative Rules): For specific monetary limits, notifications, and guidelines.

    [Instructions]
    1. If the action has clear illegal potential, prioritize 'law'.
    2. If specific figures or procedures are important, definitely include 'admrul'.
    3. List databases in order of importance.
    4. Add 'Focus Keywords' if there are additional topics to search (e.g., fintech, sharing economy).
    
    Output JSON:
    {{
        "rationale": "Reason for this strategy (English)",
        "databases": ["law", "admrul"],
        "focus_keywords": ["KoreanKeyWord1", "KoreanKeyWord2"]
    }}
    
    [Important]
    "focus_keywords" MUST be in KOREAN.
    """

    EXPANSION_PROMPT = """
    Regarding the user's action '{action}' (Target: {object}), extract 5 'single legal keywords' for searching.
    
    [Important]
    The user action is provided in English/Korean, but the **Keywords MUST be in KOREAN** for the South Korean Law Database.
    If the action is in English, translate the legal concepts to Korean first.

    [Constraints]
    1. Must be a single word, not a compound noun. (e.g., "Foreign Exchange" -> "ì™¸êµ­í™˜", "Personal Information" -> "ê°œì¸ì •ë³´")
    2. Must be a noun without particles. (No 'ì„', 'ë¥¼', 'ì˜')
    3. Output strictly a JSON list of Korean strings.
    """

    SELECTOR_PROMPT = """
    User Action: "{action}"

    Below is a list of candidate laws/rules:
    {candidates}

    [Instructions]
    From the [Candidates] list above, select up to 10 items most relevant to the user's action.
    DO NOT create or add new law names that are not in the list.
    Output ONLY the exact text from the [Candidates] list as a JSON list.
    """

    KEYWORD_GEN_PROMPT = """
    User Action: "{action}"
    Infer 5 core legal issue keywords (search terms) that might be problematic for the action.
    
    [Important]
    **Keywords MUST be in KOREAN** (Hangul).
    The search engine only understands Korean legal terms.

    These should not be simple nouns (e.g., 'transaction'), but words that can find specific illegal acts or penalty provisions.
    Examples: "í™˜ì¹˜ê¸°", "ë¬´ë“±ë¡", "ë¶ˆë²•í™˜ì „", "ìœ ì‚¬ìˆ˜ì‹ "
    Output as a JSON list of Korean strings.
    """

    CRITIC_PROMPT = """
    [Review Mode]
    User Action: "{action}"
    Current list of secured legal evidence:
    {evidence_summary}

    Q: Can the user's action be judged through 'legal interpretation' based solely on the evidence above?

    [PASS Criteria (Relaxed)]
    - If there are similar regulations or general principles, even without explicit provisions â†’ PASS
    - If there are related limit concepts or reporting obligations, even without specific amounts â†’ PASS
    - If reasonable inference is possible from laws alone, even without precedents â†’ PASS
    - If there are prohibition/restriction clauses, even without penalty clauses â†’ PASS

    [FAIL Criteria]
    - If no relevant laws were found at all (completely different field)
    - If too abstract to interpret in any direction

    If sufficient, output "PASS". If clearly insufficient, output "FAIL" along with suggested additional keywords.

    Output JSON Format:
    {{
        "status": "PASS" | "FAIL",
        "reason": "Reason (English)",
        "new_keywords": ["keyword1", "keyword2"]
    }}
    """

    PROMPTS = {
        "law": "[Law Analysis] Core: Find clauses (Article X) in this law that prohibit or restrict the user's action.",
        "admrul": "[Administrative Rule Analysis] Core: Find specific approval criteria, monetary limits, and reporting procedure figures.",
        "prec": "[Precedent Analysis] Core: Find the gist and applied legal principles of judgments (guilty/not guilty) for similar actions."
    }

    def __init__(self):
        # [NEW] ë¶„ì„ ê²°ê³¼ ìºì‹œ (LLM ë¹„ìš© ì ˆê°)
        self._analysis_cache = {}

    async def _plan_search(self, action: AtomicAction) -> SearchStrategy:
        prompt = self.STRATEGY_PROMPT.format(action=action.action)
        # [MODEL: GPT-4o] ê²€ìƒ‰ 'ì „ëµ' ìˆ˜ë¦½ì€ ê³ ì§€ëŠ¥ í•„ìš”
        response = await llm_client.generate(prompt, "", model="gpt-4o", max_tokens=512)
        try:
            data = json_repair.loads(response)
            return SearchStrategy(**data)
        except Exception as e:
            print(f"      âš ï¸ Strategy Error: {e}, Defaulting to full search.")
            return SearchStrategy(rationale="Error in strategy planning", databases=["law", "admrul", "prec"])

    def _clean_keywords(self, keywords: List[str]) -> List[str]:
        cleaned = []
        for k in keywords:
            k = re.sub(r'\(.*?\)', '', k)
            k = re.sub(r'\s*ì œ\d*O*ì¡°.*', '', k)
            k = re.sub(r'[a-zA-Z]', '', k)
            k = k.strip()
            if len(k) >= 2: cleaned.append(k)
        return cleaned

    async def _expand_query(self, action: AtomicAction) -> List[str]:
        # 1. ë²•ë ¹ëª… ì¶”ì¶œ (ë‹¨ì¼ í‚¤ì›Œë“œ)
        prompt = self.EXPANSION_PROMPT.format(action=f"{action.action}", object=action.object)
        # [MODEL: GPT-4o-mini] í‚¤ì›Œë“œ ì¶”ì¶œì€ ë‹¨ìˆœ ì‘ì—…
        response = await llm_client.generate(prompt, "", model="gpt-4o-mini", max_tokens=256)
        try:
            parsed = json_repair.loads(response)
            return self._clean_keywords(parsed if isinstance(parsed, list) else [str(parsed)])[:5]
        except:
            return []

    async def _generate_prec_keywords(self, action: AtomicAction) -> List[str]:
        # 2. íŒë¡€/ì •ë°€ ê²€ìƒ‰ìš© êµ¬ì²´ì  í‚¤ì›Œë“œ ì¶”ì¶œ
        prompt = self.KEYWORD_GEN_PROMPT.format(action=action.action)
        # [MODEL: GPT-4o-mini] í‚¤ì›Œë“œ ì¶”ì¶œì€ ë‹¨ìˆœ ì‘ì—…
        response = await llm_client.generate(prompt, "", model="gpt-4o-mini", max_tokens=256)
        try:
            parsed = json_repair.loads(response)
            return self._clean_keywords(parsed if isinstance(parsed, list) else [str(parsed)])[:5]
        except:
            return []

    async def _select_best_candidates(self, candidates: List[Dict[str, Any]], action_text: str) -> List[Dict[str, Any]]:
        if not candidates: return []

        # LLMì—ê²Œ í›„ë³´êµ° ì „ë‹¬í•˜ì—¬ ì„ íƒ ìš”ì²­
        candidate_names = [c.get('ë²•ë ¹ëª…í•œê¸€') or c.get('í–‰ì •ê·œì¹™ëª…') for c in candidates]
        prompt = self.SELECTOR_PROMPT.format(action=action_text, candidates=candidate_names)

        try:
            # [MODEL: GPT-4o-mini] ëª©ë¡ ì¤‘ ì„ íƒ(Selection)ì€ minië„ ì˜í•¨
            response = await llm_client.generate(prompt, "", model="gpt-4o-mini", max_tokens=512)
            selected_names = json_repair.loads(response)
            if not isinstance(selected_names, list): selected_names = [str(selected_names)]

            print(f"      ğŸ¤– [Selector] LLM ì„ íƒ: {selected_names}")

            final_items = []
            for name in selected_names:
                # ì´ë¦„ì´ ì¼ì¹˜í•˜ëŠ” ì•„ì´í…œ ì°¾ê¸° (ë¶€ë¶„ ì¼ì¹˜ (in)ëŠ” ìœ„í—˜í•  ìˆ˜ ìˆìœ¼ë‹ˆ, ìµœëŒ€í•œ ì •í™•íˆ ë§¤ì¹­ ì‹œë„)
                for item in candidates:
                    item_name = item.get('ë²•ë ¹ëª…í•œê¸€') or item.get('í–‰ì •ê·œì¹™ëª…')
                    # LLMì´ ì´ë¦„ì„ ì¡°ê¸ˆ ì˜ë¼ì„œ ë§í•  ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ contains ì²´í¬
                    if name in item_name or item_name in name:
                        final_items.append(item)
                        break
            return final_items if final_items else candidates[:10] # Fallback increased
        except Exception as e:
            print(f"      âš ï¸ Selector Error: {e}")
            return candidates[:10]

    async def _critique(self, action_text: str, evidence: List[str]) -> Dict[str, Any]:
        summary = "\n".join(evidence) if evidence else "None"
        prompt = self.CRITIC_PROMPT.format(action=action_text, evidence_summary=summary)
        # [MODEL: GPT-4o] ì¶©ë¶„í•œì§€ íŒë‹¨(Reasoning)í•˜ëŠ” Criticì€ ë˜‘ë˜‘í•´ì•¼ í•¨ (í™˜ê° ë°©ì§€)
        response = await llm_client.generate(prompt, "", model="gpt-4o", max_tokens=512)
        try:
            val = json_repair.loads(response)
            if not isinstance(val, dict): return {"status": "PASS", "new_keywords": []}
            return val
        except:
            return {"status": "PASS", "new_keywords": []}

    async def _search_phase(self, keywords: List[str], prec_keywords: List[str], action: AtomicAction, strategy: SearchStrategy, on_log: Optional[Callable[[str], Any]] = None) -> List[Tuple[str, str, str, str, Any]]:
        collected_raw_data = []
        found_law_titles = []
        
        async def log(msg):
            if on_log: await on_log(msg)

        # Merge focus keywords from strategy
        if strategy.focus_keywords:
            keywords.extend(strategy.focus_keywords)
            prec_keywords.extend(strategy.focus_keywords)
            
        target_dbs = strategy.databases
        await log(f"      ğŸ¯ [ì „ëµ] ëŒ€ìƒ DB: {target_dbs}")

        # [Phase 1.1] 2ë‹¨ê³„ ë²•ë ¹ ê²€ìƒ‰ (í›„ë³´êµ° ì„ ì • -> LLM ì„ íƒ -> ë³¸ë¬¸ ê²€ìƒ‰)
        if 'law' in target_dbs:
            await log(f"      ğŸ“¡ [1ë‹¨ê³„] í˜„í–‰ë²•ë ¹ í›„ë³´ ê²€ìƒ‰: {keywords}")

        if not keywords: keywords = []

        # 1. eflawë¡œ í›„ë³´êµ° ë¦¬ìŠ¤íŠ¸ì—…
        search_tasks = [law_api.search_list('eflaw', kw, display=10) for kw in keywords] # [Update] display=10
        candidate_items = []

        if search_tasks:
            results = await asyncio.gather(*search_tasks)
            for res, kw in zip(results, keywords):
                if res:
                    # í‚¤ì›Œë“œë³„ ìƒìœ„ 30ê°œ í›„ë³´ ìˆ˜ì§‘ (ê¸°ì¡´ 10ê°œ -> 30ê°œ í™•ì¥)
                    candidates = res[:30]
                    # print(f"        -> '{kw}' ê²°ê³¼: {[c.get('ë²•ë ¹ëª…í•œê¸€') for c in candidates]}")
                    candidate_items.extend(candidates)

        # 2. í›„ë³´êµ° ì¤‘ë³µ ì œê±°
        seen_ids = set()
        unique_candidates = []
        for item in candidate_items:
            # eflaw ê²°ê³¼ëŠ” 'law' íƒœê·¸ì˜€ìœ¼ë¯€ë¡œ 'ë²•ë ¹ëª…í•œê¸€' ì‚¬ìš©
            name = item.get('ë²•ë ¹ëª…í•œê¸€')
            if name and name not in seen_ids:
                seen_ids.add(name)
                unique_candidates.append(item)
        
        await log(f"        -> {len(unique_candidates)}ê°œ ë²•ë ¹ í›„ë³´ ë°œê²¬")

        # 3. LLM Selectorë¥¼ í†µí•œ ìµœì¢… ì„ ì •
        if unique_candidates:
            target_candidates = await self._select_best_candidates(unique_candidates, action.action)
        else:
            target_candidates = []

        found_law_titles = [item.get('ë²•ë ¹ëª…í•œê¸€') for item in target_candidates]
        await log(f"      ğŸ‘‰ 2ë‹¨ê³„ ë²•ë ¹ ë³¸ë¬¸ ì¡°íšŒ: {len(found_law_titles)}ê±´")

        # 4. ë³¸ë¬¸ ìƒì„¸ ì¡°íšŒ
        fetch_tasks = [law_api.get_content_from_item(item) for item in target_candidates]
        if fetch_tasks:
            contents = await asyncio.gather(*fetch_tasks)
            for item, (content, url, raw_data) in zip(target_candidates, contents):
                title = item.get('ë²•ë ¹ëª…í•œê¸€')
                collected_raw_data.append(('law', title, content, url, raw_data))

        # [Phase 2] AdmRul Search (2-Stage with Selector)
        # ì‚¬ìš©ì ìš”ì²­: í–‰ì •ê·œì¹™ë„ 'ë‹¨ì–´(Keywords)'ë¡œ ê²€ìƒ‰í•´ì•¼ ë” ë„“ì€ ë²”ìœ„ë¥¼ í¬ê´„ ê°€ëŠ¥
        if 'admrul' in target_dbs:
            await log(f"      ğŸ“¡ í–‰ì •ê·œì¹™ ê²€ìƒ‰ ì¤‘...")
            admrul_queries = keywords[:3]

            if admrul_queries:
                # print(f"      ğŸ“¡ [1ë‹¨ê³„] í–‰ì •ê·œì¹™(admrul) í›„ë³´ ê²€ìƒ‰: {admrul_queries}")
                
                adm_tasks = [law_api.search_list('admrul', kw, display=30, nw=1) for kw in admrul_queries]
                adm_raw_results = await asyncio.gather(*adm_tasks)
                
                adm_candidates = []
                adm_seen = set()
                
                for res in adm_raw_results:
                    for item in res:
                        name = item.get('í–‰ì •ê·œì¹™ëª…')
                        if name and name not in adm_seen:
                            adm_seen.add(name)
                            adm_candidates.append(item)

            if adm_candidates:
                target_admruls = await self._select_best_candidates(adm_candidates, action.action)
            else:
                target_admruls = []
            
            await log(f"      ğŸ‘‰ í–‰ì •ê·œì¹™ ë³¸ë¬¸ ì¡°íšŒ: {len(target_admruls)}ê±´")

            # ë³¸ë¬¸ ìƒì„¸ ì¡°íšŒ
            adm_fetch_tasks = [law_api.get_content_from_item(item) for item in target_admruls]
            if adm_fetch_tasks:
                adm_contents = await asyncio.gather(*adm_fetch_tasks)
                for item, (content, url, raw_data) in zip(target_admruls, adm_contents):
                    collected_raw_data.append(('admrul', item.get('í–‰ì •ê·œì¹™ëª…'), content, url, raw_data))

        # [Phase 3] Precedent Search (Multi-Strategy)
        if 'prec' in target_dbs:
            await log(f"      ğŸ“¡ íŒë¡€ ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘...")
            # print(f"      ğŸ“¡ íŒë¡€ ê²€ìƒ‰: í‚¤ì›Œë“œ={prec_keywords}, ëŒ€ìƒë²•ë ¹={found_law_titles}")
            
            prec_tasks = []
            
            # Strategy 1: ë²•ë ¹ëª… + í‚¤ì›Œë“œ ì¡°í•© (JO íŒŒë¼ë¯¸í„° í™œìš©)
            for title in found_law_titles[:2]: # Top 2 Law only
                for kw in prec_keywords:
                    prec_tasks.append(law_api.search_list('prec', query=kw, JO=title, display=30))

            # Strategy 2: í‚¤ì›Œë“œ ë‹¨ë… ê²€ìƒ‰ (Global)
            for kw in prec_keywords:
                prec_tasks.append(law_api.search_list('prec', query=kw, display=30))

            prec_results = []
            if prec_tasks:
                prec_results = await asyncio.gather(*prec_tasks)

            # 1. Candidate Collection (Metadata only)
            prec_candidates = []
            seen_prec_ids = set()
            
            for res in prec_results:
                for item in res: # All items from search
                    # íŒë¡€ëŠ” 'íŒë¡€ì¼ë ¨ë²ˆí˜¸'ê°€ ê³ ìœ  ID
                    p_id = item.get('íŒë¡€ì¼ë ¨ë²ˆí˜¸')
                    if p_id and p_id not in seen_prec_ids:
                        seen_prec_ids.add(p_id)
                        # Selectorë¥¼ ìœ„í•´ 'ë²•ë ¹ëª…í•œê¸€' í•„ë“œë¥¼ ì‚¬ê±´ëª…ìœ¼ë¡œ ë§¤í•‘ (Selectorê°€ ë²•ë ¹ëª…í•œê¸€ì„ ë´„)
                        item['ë²•ë ¹ëª…í•œê¸€'] = f"[íŒë¡€] {item.get('íŒë¡€ë‚´ìš©') or item.get('ì‚¬ê±´ëª…')}"
                        prec_candidates.append(item)
            
            # print(f"      ğŸ” íŒë¡€ í›„ë³´êµ°: {len(prec_candidates)}ê±´ ìˆ˜ì§‘ë¨")

            # 2. LLM Selector (Filter)
            if prec_candidates:
                # íŒë¡€ëŠ” ì œëª©ë§Œìœ¼ë¡œ íŒë‹¨í•˜ê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆìœ¼ë‚˜, ì‚¬ê±´ëª…ì— í•µì‹¬ì´ í¬í•¨ë¨.
                # [Optimization] Selector can handle up to MAX_ANALYSIS_DOCS
                target_precs = await self._select_best_candidates(prec_candidates[:MAX_ANALYSIS_DOCS], action.action)
            else:
                target_precs = []
            
            await log(f"      ğŸ‘‰ íŒë¡€ ë³¸ë¬¸ ì¡°íšŒ: {len(target_precs)}ê±´")

            # 4. ë³¸ë¬¸ ìƒì„¸ ì¡°íšŒ
            prec_fetch_tasks = [law_api.get_content_from_item(item) for item in target_precs]
            if prec_fetch_tasks:
                prec_contents = await asyncio.gather(*prec_fetch_tasks)
                for item, (content, url, raw_data) in zip(target_precs, prec_contents):
                    title = item.get('ë²•ë ¹ëª…í•œê¸€')
                    collected_raw_data.append(('prec', title, content, url, raw_data))

        # [Limit] Max Documents to prevent token explosion
        if len(collected_raw_data) > MAX_ANALYSIS_DOCS:
            await log(f"      âœ‚ï¸ ë¬¸ì„œ ê³¼ë‹¤ë¡œ ìƒìœ„ {MAX_ANALYSIS_DOCS}ê±´ë§Œ ë¶„ì„í•©ë‹ˆë‹¤.")
            collected_raw_data = collected_raw_data[:MAX_ANALYSIS_DOCS]

        return collected_raw_data

    async def _analyze_full_text(self, text: str, action: AtomicAction, category: str, title: str, url: str, raw_data: Any) -> List[DocumentReview]:
        """
        ë¬¸ì„œ ì „ì²´ë¥¼ ìˆœíšŒí•˜ë©° í•µì‹¬ ë‚´ìš© ì¶”ì¶œ (Full-Text Chunking & Structured Review)
        ë†“ì¹˜ëŠ” ì¡°í•­ì´ ì—†ë„ë¡ ì „ì²´ë¥¼ ë‹¤ í›‘ì–´ë´„.
        """
        # [Cache] Unique ID ê¸°ë°˜ ìºì‹± (Title ì‚¬ìš© ì¤‘ë‹¨)
        doc_id = law_api._get_unique_id(raw_data)
        # ë§Œì•½ raw_dataê°€ ì—†ê±°ë‚˜ íŒŒì‹± ì‹¤íŒ¨ì‹œ, ì„ì‹œë¡œ title+url í•´ì‹œ ì‚¬ìš©
        if doc_id == "Unknown":
            doc_id = f"{title}_{url}"
            
        cache_key = (action.action, doc_id)
        
        if cache_key in self._analysis_cache:
            # print(f"      âš¡ [Cache Hit] ID={doc_id} ì‚¬ìš©")
            cached_reviews = self._analysis_cache[cache_key]
            for r in cached_reviews: r.url = url
            return cached_reviews

        reviews = []
        
        
        # [NEW] êµ¬ì¡°ì  ë¶„ì„ (Smart Index Scanning)
        # ë²•ë ¹(law), í–‰ì •ê·œì¹™(admrul) ê·¸ë¦¬ê³  ì´ì œ íŒë¡€(prec)ë„ ì§€ì›
        if category in ['law', 'admrul', 'prec']:
            articles = law_api._parse_law_structure(raw_data)
            
            # Case 1: íŒë¡€ (Precedent) - êµ¬ì¡° ë¶„ì„ ê²°ê³¼ê°€ ìˆë‹¤ë©´ í•­ìƒ ì‚¬ìš© (í° í…ìŠ¤íŠ¸ ë°©ì§€)
            if category == 'prec' and articles:
                 # íŒë¡€ëŠ” [íŒì‹œì‚¬í•­, íŒê²°ìš”ì§€] ë§Œìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ ì¬ë¶„ì„
                 # print(f"      âš–ï¸ [Precedent] íŒë¡€ êµ¬ì¡° ë¶„ì„ (íŒì‹œì‚¬í•­/íŒê²°ìš”ì§€ ìœ„ì£¼)")
                 combined_text = "\n\n".join([f"[{a['id']}]\n{a['content']}" for a in articles])
                 # ì¬ê·€ í˜¸ì¶œí•˜ì—¬ ì§§ì€ í…ìŠ¤íŠ¸ ë¡œì§ìœ¼ë¡œ ì²˜ë¦¬
                 return await self._analyze_full_text(combined_text, action, category, title, url, {}) 

            # Case 2: ë²•ë ¹/í–‰ì •ê·œì¹™ - ì¡°ë¬¸ì´ ë„ˆë¬´ ë§ìœ¼ë©´ ëª©ì°¨ ìŠ¤ìºë‹ ìˆ˜í–‰
            if len(articles) > 5:
                # ì¡°ë¬¸ì´ ë„ˆë¬´ ë§ìœ¼ë©´ (ì˜ˆ: 5ê°œ ì´ìƒ) ëª©ì°¨ ìŠ¤ìºë‹ ìˆ˜í–‰
                print(f"      ğŸ“‘ [Index Scan] {title} - ì´ {len(articles)}ê°œ ì¡°ë¬¸ ì¤‘ ê´€ë ¨ ì¡°í•­ ì„ ë³„ ì¤‘...")
                
                # 1. ëª©ì°¨(ì œëª©)ë§Œ ì¶”ì¶œ
                toc_text = "\n".join([f"{i}. {art['id']}" for i, art in enumerate(articles)])
                
                prompt = f"""
                [Table of Contents: {title}]
                {toc_text}
                
                [User Action]
                {action.action}
                
                Select the indices (numbers) of articles that seem most relevant to the User Action.
                Select up to 5 articles. If none, return empty list.
                Output JSON: [0, 3, 12]
                """
                
                # [MODEL: GPT-4o-mini] ëª©ì°¨ ìŠ¤ìºë‹ì€ ë§¤ìš° ê°€ë²¼ì›€
                res = await llm_client.generate(prompt, "", model="gpt-4o-mini", max_tokens=128)
                try:
                    selected_indices = json_repair.loads(res)
                    if not isinstance(selected_indices, list): selected_indices = []
                    
                    target_articles = [articles[i] for i in selected_indices if isinstance(i, int) and 0 <= i < len(articles)]
                    
                    if target_articles:
                        print(f"        -> ì„ ë³„ëœ ì¡°í•­: {[a['id'] for a in target_articles]}")
                        # [NEW] ì„ ë³„ëœ ì¡°í•­ì„ ê°œë³„ì ìœ¼ë¡œ ë¶„ì„ (chunking ë°©ì§€)
                        for art in target_articles:
                            art_prompt = f"""
                            [Analysis Target: {category} - {title}]
                            [{art['id']}]
                            {art['content']}

                            [User Action]
                            {action.action}

                            Extract legal grounds related to the 'User Action' from the text and respond in JSON.
                            
                            [Target Schema]
                            {{
                                "law_name": "{title}",
                                "key_clause": "{art['id']}",
                                "status": "Prohibited | Permitted | Conditional | Neutral | Ambiguous",
                                "summary": "í•´ë‹¹ ì¡°í•­ì˜ í•µì‹¬ ë‚´ìš© ìš”ì•½ (í•œê¸€ 2ë¬¸ì¥ ì´ë‚´)"
                            }}
                            If there is no relevant content at all, set the status to 'Neutral'.
                            """
                            art_res = await llm_client.generate(art_prompt, "", model="gpt-4o-mini", max_tokens=512)
                            try:
                                art_data = json_repair.loads(art_res)
                                if art_data.get('status') != 'Neutral':
                                    rev = DocumentReview(**art_data)
                                    rev.url = url
                                    reviews.append(rev)
                            except:
                                pass
                        
                        # ìºì‹œ ì €ì¥ í›„ ë°˜í™˜ (chunking ë‹¨ê³„ë¡œ ê°€ì§€ ì•ŠìŒ)
                        self._analysis_cache[cache_key] = reviews
                        return reviews
                except Exception as e:
                    print(f"        âš ï¸ Index Scan Error: {e}, Falling back to full scan.")
                    pass # ì‹¤íŒ¨í•˜ë©´ ì•„ë˜ ì²­í¬ ë¡œì§ìœ¼ë¡œ ë„˜ì–´ê°

        # 1. í…ìŠ¤íŠ¸ê°€ ì§§ìœ¼ë©´ ë°”ë¡œ ë¶„ì„
        if len(text) < 5000:
            prompt = f"""
            [Analysis Target: {category} - {title}]
            {text}

            [User Action]
            {action.action}

            Extract legal grounds related to the 'User Action' from the text and respond in JSON.
            
            [Target Schema]
            {{
                "law_name": "{title}",
                "key_clause": "ê´€ë ¨ ì¡°í•­ (ì˜ˆ: ì œ3ì¡° ì œ1í•­) ì—†ìœ¼ë©´ ë¹ˆì¹¸",
                "status": "ê¸ˆì§€ | í—ˆìš© | ì¡°ê±´ë¶€ | ì¤‘ë¦½ | ë¶ˆëª…í™•",
                "summary": "í•´ë‹¹ ì¡°í•­ì˜ í•µì‹¬ ë‚´ìš© ìš”ì•½ (í•œê¸€ 2ë¬¸ì¥ ì´ë‚´)"
            }}
            If there is no relevant content at all, set the status to 'ì¤‘ë¦½'.
            """
            # [MODEL: GPT-4o-mini] ì½ì–´ì•¼ í•  ì–‘ì´ ê°€ì¥ ë§ì€ ë¶€ë¶„. mini ì‚¬ìš© í•„ìˆ˜ (ë¹„ìš© ì ˆê°)
            res = await llm_client.generate(prompt, "", model="gpt-4o-mini", max_tokens=512)
            try:
                data = json_repair.loads(res)
                if data.get('status') != 'ì¤‘ë¦½':
                    rev = DocumentReview(**data)
                    rev.url = url
                    reviews.append(rev)
            except:
                pass
            return reviews

        # 2. í…ìŠ¤íŠ¸ê°€ ê¸¸ë©´ ë¶„í•  ì²˜ë¦¬ (Chunking) - Full Scan
        chunk_size = 4000
        overlap = 300
        chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size - overlap)]
        
        print(f"      ğŸ§© [Chunking] {title} - {len(chunks)}ê°œ ì¡°ê°ìœ¼ë¡œ ë¶„í•  ë¶„ì„ ì¤‘...")

        tasks = []
        for i, chunk in enumerate(chunks):
            prompt = f"""
            [Analysis Target: {category} - {title} (Part {i+1}/{len(chunks)})]
            {chunk}

            [User Action]
            {action.action}

            If there are legal grounds (prohibition, permission, penalty, etc.) related to the 'User Action' in this text chunk, extract them.
            If it's difficult to judge due to broken context, set the status to 'Neutral'.
            
            [Target Schema]
            {{
                "law_name": "{title}",
                "key_clause": "ì¡°í•­ ë²ˆí˜¸",
                "status": "Prohibited | Permitted | Conditional | Neutral",
                "summary": "ìš”ì•½"
            }}
            """
            # [MODEL: GPT-4o-mini] ëŒ€ëŸ‰ì˜ Chunk ì²˜ë¦¬
            tasks.append(llm_client.generate(prompt, "", model="gpt-4o-mini", max_tokens=512))

        results = await asyncio.gather(*tasks)

        for res in results:
            try:
                data = json_repair.loads(res)
                if data.get('status') != 'Neutral':
                     # ì¤‘ë³µ ì œê±° (ê°™ì€ ì¡°í•­ì´ ì—¬ëŸ¬ ì²­í¬ì— ê±¸ì¹  ìˆ˜ ìˆìŒ)
                     rev = DocumentReview(**data)
                     rev.url = url
                     reviews.append(rev)
            except:
                pass
        
        # [Cache] ê²°ê³¼ ì €ì¥
        self._analysis_cache[cache_key] = reviews
        return reviews

    async def _extract_evidence(self, raw_data: List[Tuple[str, str, str, str, Any]], action: AtomicAction) -> List[DocumentReview]:
        if not raw_data: return []
        
        doc_count = len(raw_data)
        doc_titles = [f"[{c}] {t}" for c, t, _, _, _ in raw_data[:5]]
        etc_text = f" ì™¸ {doc_count-5}ê±´" if doc_count > 5 else ""

        print(f"\n      ğŸ“ [ì •ë°€ ë¶„ì„] ì´ {doc_count}ê±´ì˜ ë¬¸ê±´ì„ ì „ìˆ˜ ì¡°ì‚¬í•©ë‹ˆë‹¤. (LLM Reading...)")
        print(f"         ëŒ€ìƒ: {', '.join(doc_titles)}{etc_text}")
        
        tasks = []
        valid_reviews = []

        for category, title, text, url, raw_data_item in raw_data:
            if not text or len(text) < 50: continue
            # [Update] ì „ì²´ í…ìŠ¤íŠ¸ ë¶„ì„ ìš”ì²­ (List[DocumentReview] ë°˜í™˜)
            tasks.append(self._analyze_full_text(text, action, category, title, url, raw_data_item))

        if tasks:
            results = await asyncio.gather(*tasks)
            for res_list in results:
                # ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ì´ë¯€ë¡œ í™•ì¥
                valid_reviews.extend(res_list)

        return valid_reviews

    async def _process_action(self, action: AtomicAction) -> List[DocumentReview]:
        print(f"\\n  ğŸ¬ [Action ì‹œì‘] {action.action}")

        # 1. Keyword Generation
        law_names = await self._expand_query(action)
        prec_keywords = await self._generate_prec_keywords(action)
        print(f"    1ï¸âƒ£  [ì´ˆê¸° í‚¤ì›Œë“œ] ë²•ë ¹: {law_names} | íŒë¡€: {prec_keywords}")

        # 1.5 Strategy Planning
        strategy = await self._plan_search(action)
        print(f"    ğŸ§  [ì „ëµ ìˆ˜ë¦½] {strategy.rationale}")
        print(f"       -> ëŒ€ìƒ DB: {strategy.databases} | í•µì‹¬ì–´: {strategy.focus_keywords}")

        final_evidence = []

        # 2. Loop (Initial + Retry)
        max_retries = 1
        for attempt in range(max_retries + 1):
            is_retry = attempt > 0
            prefix = "ğŸ”„ [ì¬ê²€ìƒ‰]" if is_retry else "ğŸš€ [1ì°¨ ê²€ìƒ‰]"
            print(f"    {prefix} ì‹œì‘...")

            # Search
            raw_data = await self._search_phase(law_names, prec_keywords, action, strategy)

            # Extract (Structured)
            new_reviews = await self._extract_evidence(raw_data, action)
            
            # Deduplicate by law_name + key_clause
            existing_keys = set(f"{r.law_name}-{r.key_clause}" for r in final_evidence)
            for r in new_reviews:
                key = f"{r.law_name}-{r.key_clause}"
                if key not in existing_keys:
                    final_evidence.append(r)
                    existing_keys.add(key)

            # Critique (Needs string for criticism)
            evidence_summary = [f"[{r.status}] {r.law_name} {r.key_clause}: {r.summary}" for r in final_evidence]
            critic_res = await self._critique(action.action, evidence_summary)
            print(f"      ğŸ§ [Critic í‰ê°€] {critic_res.get('status')} : {critic_res.get('reason')}")

            if critic_res.get('status') == 'PASS':
                print("      -> ì¶©ë¶„í•œ ê·¼ê±° í™•ë³´. ê²€ìƒ‰ ì¢…ë£Œ.")
                break

            if is_retry:
                print("      -> ì¬ê²€ìƒ‰í–ˆìœ¼ë‚˜ ì—¬ì „íˆ ë¶€ì¡±í•¨. ì¢…ë£Œ.")
                break

            # Prepare Retry
            new_kws = critic_res.get('new_keywords', [])
            if new_kws:
                print(f"      -> âš ï¸ ë¶€ì¡±í•¨! Critic ì œì•ˆ í‚¤ì›Œë“œë¡œ ì¬ì‹œë„: {new_kws}")
                law_names = [k for k in new_kws if 'ë²•' in k or 'Act' in k]
                prec_keywords = [k for k in new_kws if 'ë²•' not in k and 'Act' not in k]
                if not law_names and not prec_keywords: break
            else:
                break

        print(f"      -> âœ… ìµœì¢… í™•ë³´ëœ ê·¼ê±°: {len(final_evidence)}ê±´")
        return final_evidence

    async def execute(self, scenario: Scenario, on_log: Optional[Callable[[str], Any]] = None) -> Tuple[LegalEvidence, List[DocumentReview]]:
        async def log(msg):
            if on_log: await on_log(msg)
            
        await log(f"\n[3-1] Investigator: Analyzing '{scenario.name}'...")
        
        # 1. Action ë¶„í•´ ë° ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½
        all_reviews = []
        
        for action in scenario.actions:
            await log(f"\n    ğŸ§ Investigating Action: {action.action}")
            
            # (1) ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½
            strategy = await self._plan_search(action)
            await log(f"      ğŸ“‹ ê²€ìƒ‰ ì „ëµ: {strategy.rationale}")
            
            # (2) í‚¤ì›Œë“œ í™•ì¥
            keywords = await self._expand_query(action)
            prec_keywords = await self._generate_prec_keywords(action)
            
            # (3) ê²€ìƒ‰ ë° ë²•ì  ê·¼ê±° ì¶”ì¶œ (Retry ë¡œì§ í¬í•¨)
            raw_data = await self._search_phase(keywords, prec_keywords, action, strategy, on_log=on_log)
            
            # Count Types
            cnt_law = sum(1 for r in raw_data if r[0] == 'law')
            cnt_prec = sum(1 for r in raw_data if r[0] == 'prec')
            cnt_adm = sum(1 for r in raw_data if r[0] == 'admrul')
            await log(f"      ğŸ“Š ìˆ˜ì§‘ëœ ìë£Œ: ë²•ë ¹ {cnt_law}ê±´, íŒë¡€ {cnt_prec}ê±´, í–‰ì •ê·œì¹™ {cnt_adm}ê±´")

            reviews = await self._extract_evidence(raw_data, action)
            
            # (4) ê²€ì¦ (Critic)
            docs_text = [r.summary for r in reviews]
            critique = await self._critique(action.action, docs_text)
            
            if critique.get("status") == "RETRY":
                await log(f"      ğŸ”„ ì¬ê²€ìƒ‰ ìš”ì²­: {critique.get('reason')}")
                # print(f"      ğŸ”„ ì¬ê²€ìƒ‰ ìš”ì²­: {critique.get('reason')}")
                new_kws = critique.get("new_keywords", [])
                # ê°„ë‹¨íˆ ì¶”ê°€ ê²€ìƒ‰ ìˆ˜í–‰ (Strategy ë¬´ì‹œí•˜ê³  í‚¤ì›Œë“œ ì¤‘ì‹¬)
                raw_data_retry = await self._search_phase(new_kws, new_kws, action, strategy, on_log=on_log)
                reviews_retry = await self._extract_evidence(raw_data_retry, action)
                reviews.extend(reviews_retry)

            all_reviews.extend(reviews)

        # ì¤‘ë³µ ì œê±°
        unique_reviews = []
        seen = set()
        for r in all_reviews:
            key = (r.law_name, r.key_clause)
            if key not in seen:
                seen.add(key)
                unique_reviews.append(r)
        
        # [Limit] Hard limit to 50 (Total)
        if len(unique_reviews) > 50:
             await log(f"      âœ‚ï¸ ì „ì²´ ìˆ˜ì§‘ ìë£Œ {len(unique_reviews)}ê±´ ì¤‘ ìƒìœ„ 50ê±´ë§Œ ì‚¬ìš©í•˜ì—¬ ë¶„ì„í•©ë‹ˆë‹¤.")
             unique_reviews = unique_reviews[:50]

        # Format for Auditor
        summary_lines = []
        for r in unique_reviews:
            icon = "ğŸ”´" if r.status == 'Prohibited' else "ğŸŸ¢" if r.status == 'Permitted' else "ğŸŸ¡"
            link_md = f"[ì›ë¬¸]({r.url})" if r.url else ""
            summary_lines.append(f"{icon} [{r.status}] {r.law_name} {r.key_clause} | {r.summary} {link_md}")
            
        evidence = LegalEvidence(
            relevant_laws=summary_lines,
            summary=f"ë°œê²¬ëœ ë²•ì  ê·¼ê±° {len(unique_reviews)}ê±´"
        )
        await log(f"âœ… [Investigator] ì´ {len(unique_reviews)}ê±´ì˜ ê·¼ê±° ìˆ˜ì§‘ ì™„ë£Œ.\n")
        return evidence, unique_reviews

investigator = Investigator()
print("âœ… Investigator Updated with Detailed Logging & Critic Loop.")

from typing import Optional

# [ìˆ˜ì • 1] RiskReport ëª¨ë¸ì— ê¸°ë³¸ê°’(default) ì¶”ê°€í•˜ì—¬ ì—ëŸ¬ ë°©ì§€
class RiskReport(BaseModel):
    verdict: str = Field(default="Caution", description="Risk Level: Safe | Caution | Danger")
    summary: str = Field(default="íŒë‹¨ ë³´ë¥˜", description="Detailed Verdict Summary")
    citation: str = Field(default="êµ¬ì²´ì  ì¡°í•­ ì—†ìŒ", description="Legal Citation")
    key_issues: List[str] = Field(default_factory=list, description="Key legal issues identified")


# [ìˆ˜ì • 2] Auditor íŒŒì‹± ë¡œì§ ê°•í™”
class AdversarialDebate:
    """
    Multi-Agent Debate System: Prosecutor vs. Defense -> Judge
    Includes Rebuttal & Reflexion (Self-Correction) phases.
    """

    # [Update] Risk assessment perspective (not legal judgment)
    PROSECUTOR_PROMPT = """
    You are a Risk Assessment Specialist focusing on legal compliance risks.
    Based on the scenario and evidence, identify potential legal risks and compliance issues.
    Language: English.
    
    [Scenario]
    {scenario}

    [Evidence]
    {evidence}
    """

    DEFENSE_PROMPT = """
    You are a Business Innovation Consultant.
    Based on the scenario and evidence, identify opportunities, regulatory exceptions, and mitigation strategies.
    Language: English.

    [Scenario]
    {scenario}
    
    [Evidence]
    {evidence}
    """

    REBUTTAL_PROMPT = """
    You are {role}.
    Critique the opponent's argument logically.
    Opponent Argument:
    {opponent_argument}
    
    Language: English.
    """

    REFLEXION_PROMPT = """
    You are {role}.
    Refine your final argument considering the opponent's rebuttal.
    
    My Original Argument: {my_argument}
    Opponent's Rebuttal: {rebuttal}
    
    Language: English.
    """

    JUDGE_PROMPT = """
    You are a Business Risk Assessment Expert.
    Review the risk analysis and business opportunities to provide a comprehensive risk evaluation report.
    
    [Business Scenario]
    {scenario}
    
    [Risk Assessment]
    {prosecutor_final}
    
    [Opportunity Analysis]
    {defense_final}
    
    Output JSON (MUST be in Korean):
    {{
        "ìœ„í—˜ë„": "ì•ˆì „ | ì£¼ì˜ | ìœ„í—˜",
        "ì •í™•ë„": 0 ~ 100,
        "í‰ê°€ë‚´ìš©": "ë¨¼ì € ë¶„ì„ëœ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•œ í›„ (1-2ë¬¸ì¥), í•´ë‹¹ ì‚¬ì—… ëª¨ë¸ì˜ ë²•ì  ë¦¬ìŠ¤í¬ë¥¼ í‰ê°€í•˜ì„¸ìš”. êµ¬ì²´ì ì¸ ë²•ë ¹ì„ ì¸ìš©í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”. (í•œê¸€ë¡œ ì‘ì„±)",
        "ì¸ìš©ê·¼ê±°": ["ì™¸êµ­í™˜ê±°ë˜ë²• ì œ8ì¡°", "ì „ìê¸ˆìœµê±°ë˜ë²• ì œ3ì¡°", ...],
        "í‰ê°€ê²°ê³¼": "ë¦¬ìŠ¤í¬ ìš°ì„¸ | ê¸°íšŒ ìš°ì„¸",
        "ì£¼ìš”ìŸì ": ["ì£¼ìš” ë¦¬ìŠ¤í¬ ìš”ì¸ 1 (í•œê¸€)", "ì£¼ìš” ë¦¬ìŠ¤í¬ ìš”ì¸ 2 (í•œê¸€)"]
    }}
    
    [Important]
    - Use ONLY Korean field names as shown above
    - Start with a brief explanation of the analyzed scenario (1-2 sentences)
    - Focus on business risk assessment, not legal judgment
    - Provide actionable insights for the business
    """

    async def _opening_statements(self, context: str) -> Tuple[str, str]:
        print("    âš”ï¸ [Round 1] Opening Statements...")
        # [MODEL: GPT-4o-mini] í† ë¡  ë‚´ìš© ìƒì„±ì€ Text Gen ëŠ¥ë ¥ì´ë©´ ì¶©ë¶„. ë¹„ìš© ì ˆê°.
        pros_task = llm_client.generate(self.PROSECUTOR_PROMPT.format(**context), "", model="gpt-4o-mini")
        def_task = llm_client.generate(self.DEFENSE_PROMPT.format(**context), "", model="gpt-4o-mini")
        
        pros_arg, def_arg = await asyncio.gather(pros_task, def_task)
        return pros_arg.strip(), def_arg.strip()

    async def _rebuttal_round(self, pros_arg: str, def_arg: str) -> Tuple[str, str]:
        print("    âš”ï¸ [Round 2] Rebuttal (Cross-Examination)...")
        # [MODEL: GPT-4o-mini]
        # Prosecutor critiques Defense
        p_rebut_task = llm_client.generate(self.REBUTTAL_PROMPT.format(role="Prosecutor", opponent_argument=def_arg), "", model="gpt-4o-mini")
        # Defense critiques Prosecutor
        d_rebut_task = llm_client.generate(self.REBUTTAL_PROMPT.format(role="Defense Lawyer", opponent_argument=pros_arg), "", model="gpt-4o-mini")

        p_rebut, d_rebut = await asyncio.gather(p_rebut_task, d_rebut_task)
        return p_rebut.strip(), d_rebut.strip()

    async def _reflexion_round(self, pros_arg: str, def_arg: str, p_rebut: str, d_rebut: str) -> Tuple[str, str]:
        print("    ğŸ§  [Round 3] Reflexion (Self-Correction)...")
        # [MODEL: GPT-4o-mini]
        # Prosecutor refines stance based on Defense's rebuttal
        p_final_task = llm_client.generate(self.REFLEXION_PROMPT.format(role="Prosecutor", my_argument=pros_arg, rebuttal=d_rebut), "", model="gpt-4o-mini")
        # Defense refines stance based on Prosecutor's rebuttal
        d_final_task = llm_client.generate(self.REFLEXION_PROMPT.format(role="Defense Lawyer", my_argument=def_arg, rebuttal=p_rebut), "", model="gpt-4o-mini")

        p_final, d_final = await asyncio.gather(p_final_task, d_final_task)
        return p_final.strip(), d_final.strip()

    async def _render_verdict(self, scenario_text: str, p_final: str, d_final: str) -> RiskReport:
        print("    âš–ï¸ [Judge] Rendering Final Verdict...")
        prompt = self.JUDGE_PROMPT.format(
            scenario=scenario_text,
            prosecutor_final=p_final,
            defense_final=d_final
        )

        # [MODEL: GPT-4o] íŒê²°ì€ ê°€ì¥ ë˜‘ë˜‘í•œ ëª¨ë¸ì´ í•´ì•¼ í•¨. (Final Output)
        response = await llm_client.generate(prompt, "", model="gpt-4o", max_tokens=512)

        try:
            data = json_repair.loads(response)

            # í•œêµ­ì–´ í•„ë“œëª… ë§¤í•‘ (LLMì´ í•œêµ­ì–´ë¡œ ì‘ë‹µ)
            risk_level = data.get('ìœ„í—˜ë„', data.get('risk_level', 'Caution'))
            confidence = data.get('ì •í™•ë„', data.get('confidence_score', 0))
            verdict_text = data.get('í‰ê°€ë‚´ìš©', data.get('verdict', 'í‰ê°€ ë‚´ìš© ì—†ìŒ'))
            cited = data.get('ì¸ìš©ê·¼ê±°', data.get('cited_evidence', []))
            winning = data.get('í‰ê°€ê²°ê³¼', data.get('winning_side', 'í‰ê°€ ì¤‘'))
            issues = data.get('ì£¼ìš”ìŸì ', data.get('key_issues', []))

            # ì¸ìš© ê·¼ê±° í¬ë§·íŒ…
            citation_text = "\n".join(cited) if cited else "ê·¼ê±° ì—†ìŒ"

            # ìœ„í—˜ë„ ì˜ë¬¸ ë§¤í•‘ (í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜)
            risk_map = {'ì•ˆì „': 'Safe', 'ì£¼ì˜': 'Caution', 'ìœ„í—˜': 'Danger'}
            risk_level_en = risk_map.get(risk_level, risk_level)

            return RiskReport(
                verdict=risk_level_en,
                summary=f"[{winning}] {verdict_text} (ì •í™•ë„: {confidence}%)",
                citation=citation_text,
                key_issues=issues
            )
        except Exception as e:
            print(f"Judge Error: {e}")
            return RiskReport(
                verdict="Caution", 
                summary=f"í‰ê°€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}", 
                citation="", 
                key_issues=["ì‹œìŠ¤í…œ ì˜¤ë¥˜"]
            )

    async def execute(self, scenario: Scenario, evidence: LegalEvidence) -> RiskReport:
        evidence_text = "\n".join(evidence.relevant_laws)
        if not evidence_text: evidence_text = "No specific laws found."

        context = {
            "scenario": scenario.model_dump_json(),
            "evidence": evidence_text
        }

        # 1. Opening
        p_arg, d_arg = await self._opening_statements(context)
        print(f"      ğŸ—£ï¸ Prosecutor: {p_arg[:100]}...")
        print(f"      ğŸ›¡ï¸ Defense: {d_arg[:100]}...")

        # 2. Rebuttal
        p_rebut, d_rebut = await self._rebuttal_round(p_arg, d_arg)

        # 3. Reflexion
        p_final, d_final = await self._reflexion_round(p_arg, d_arg, p_rebut, d_rebut)
        print(f"      ğŸ“ Pros Final: {p_final[:100]}...")
        print(f"      ğŸ“ Def Final: {d_final[:100]}...")

        # 4. Verdict
        return await self._render_verdict(scenario.model_dump_json(), p_final, d_final)

# ì¸ìŠ¤í„´ìŠ¤ ì—…ë°ì´íŠ¸
auditor = AdversarialDebate()
print("âœ… Adversarial Debate System (Prosecutor vs Defense vs Judge) Initialized.")

# 8. Run the Pipeline

from typing import AsyncGenerator

async def run_analysis_stream(user_input: str) -> AsyncGenerator[str, None]:
    """API Streaming Response Generator"""
    queue = asyncio.Queue()

    async def log_callback(msg: str):
        await queue.put(json.dumps({"type": "log", "message": msg}) + "\n")

    async def worker():
        try:
            # Init Agents
            structurer = Structurer()
            simulator = Simulator()
            investigator = Investigator()
            auditor = AdversarialDebate()

            await log_callback("ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ. ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

            # 1. Structure
            await log_callback("ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ êµ¬ì¡°í™” (Structuring) ì§„í–‰ ì¤‘...")
            model = await structurer.execute(user_input)
            await log_callback(f"êµ¬ì¡°í™” ì™„ë£Œ: {model.project_name}")
            
            # 2. Simulate (Main Scenario)
            await log_callback("ê·œì œ ìƒŒë“œë°•ìŠ¤ ì‹œë‚˜ë¦¬ì˜¤ ì‹œë®¬ë ˆì´ì…˜ (Simulation) ì‹œì‘...")
            scenarios = await simulator.execute(model)
            main_scenario = scenarios[0] if scenarios else None
            
            if not main_scenario:
                await queue.put(json.dumps({"type": "error", "message": "ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ì‹¤íŒ¨"}) + "\n")
                return

            await log_callback("ì£¼ìš” ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ì™„ë£Œ.")

            # 3. Investigate (Pass Log Callback)
            await log_callback("ë²•ë ¹ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„ (Investigation) ìˆ˜í–‰ ì¤‘...")
            evidence, reviews = await investigator.execute(main_scenario, on_log=log_callback)
            await log_callback(f"ê²€í†  ì™„ë£Œ: {len(reviews)}ê±´ì˜ ë²•ë ¹/íŒë¡€ ë¶„ì„ë¨.")
            
            # 4. Audit
            await log_callback("AI ê°ì‚¬ê´€ ë° ë³€í˜¸ì‚¬ í† ë¡  (Adversarial Debate) ì§„í–‰ ì¤‘...")
            final_report = await auditor.execute(main_scenario, evidence)
            await log_callback("ë²•ë¥  ê²€í†  ìµœì¢… íŒê²° ë„ì¶œ ì™„ë£Œ.")
            
            # 5. Extract Unique References
            references = []
            seen_urls = set()
            for r in reviews:
                if r.url and r.url not in seen_urls:
                    references.append({"title": f"{r.law_name} {r.key_clause}", "url": r.url})
                    seen_urls.add(r.url)

            result_data = {
                "business_model": json.loads(model.model_dump_json()),
                "scenario": json.loads(main_scenario.model_dump_json()),
                "evidence": [json.loads(r.model_dump_json()) for r in reviews],
                "verdict": json.loads(final_report.model_dump_json()),
                "references": references
            }
            
            await queue.put(json.dumps({"type": "result", "data": result_data}) + "\n")

        except Exception as e:
            print(f"Worker Error: {e}")
            await queue.put(json.dumps({"type": "error", "message": str(e)}) + "\n")
        finally:
            await queue.put(None) # Sentinel

    # Start worker on background
    asyncio.create_task(worker())

    # Consume logs
    while True:
        item = await queue.get()
        if item is None:
            break
        yield item

async def run_analysis(user_input: str) -> Dict[str, Any]:
    # Legacy wrapper if needed, or for testing
    result = None
    async for chunk in run_analysis_stream(user_input):
        data = json.loads(chunk)
        if data["type"] == "result":
            result = data["data"]
    return result

async def run_demo():
    print("âœ… Investigator Updated with Detailed Logging & Critic Loop.")
    print("âœ… Adversarial Debate System (Prosecutor vs Defense vs Judge) Initialized.")

    user_input = "ë¹Œë¼ë‚˜ ì£¼íƒ ê±°ì£¼ìê°€ ì¶œê·¼í•œ ì‹œê°„ ë™ì•ˆ ìì‹ ì˜ ë¹ˆ ì£¼ì°¨ë©´ì„ ì™¸ë¶€ì¸ì—ê²Œ ìœ ë£Œë¡œ ëŒ€ì—¬í•´ì£¼ëŠ” IoT ì£¼ì°¨ ê³µìœ  ì„œë¹„ìŠ¤"
    print(f"User Idea: {user_input}")

    print("\n--- Streaming Output ---")
    async for chunk in run_analysis_stream(user_input):
        data = json.loads(chunk)
        if data['type'] == 'log':
            print(f"LOG: {data['message']}")
        elif data['type'] == 'result':
            result = data['data']
            print("\n" + "="*50)
            print("   ğŸ“¢ [FINAL VERDICT] REPORT")
            print("="*50)
            
            verdict = result["verdict"]
            print(f"\nğŸ† íŒê²°: {verdict.get('verdict')}")
            print(f"ğŸ“ ìš”ì•½: {verdict.get('summary')}")
            print(f"\nâš–ï¸ ì£¼ìš” ìŸì :")
            for issue in verdict.get('key_issues', []):
                print(f" - {issue}")
                
            print(f"\nğŸ”— ì°¸ê³  ë¬¸í—Œ:")
            for ref in result["references"]:
                print(f" - {ref['title']}: {ref['url']}")

if __name__ == '__main__':
    asyncio.run(run_demo())
